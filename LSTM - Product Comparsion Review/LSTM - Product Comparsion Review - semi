{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1p03sk2EPv1QAhfC7513PLtg7W8qyYH3V","authorship_tag":"ABX9TyOouAFCcaLO4NoP4N+l+kV0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Data Sampling"],"metadata":{"id":"sahNfYZRCCXi"}},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"pnoaocekiH0h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_excel('/content/drive/MyDrive/Dissertation/Original.xlsx')\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":473},"id":"2MPuhKg8jBum","executionInfo":{"status":"ok","timestamp":1676696414359,"user_tz":-480,"elapsed":133428,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}},"outputId":"1aeca782-8da8-4fe2-eaf3-91b7086de807"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   review_cnt is_hot        publish_time                        id  \\\n","0        84.0    NaN 2021-02-10 12:10:00  60235cc900000000010282bf   \n","1        76.0    NaN 2021-02-06 12:05:00  601e158f000000000101f856   \n","2         0.0     爆文 2021-02-16 14:33:00  602b6729000000002103e7ff   \n","3      1182.0     爆文 2021-02-10 17:00:00  6023a0c7000000002103ecd0   \n","4       523.0     爆文 2021-02-03 22:32:00  601ab3fd000000000101f4b2   \n","\n","  is_original                                           pic_urls profilers_id  \\\n","0        普通笔记  [http://sns-img-qc.xhscdn.com/e12a7183-0d2b-3c...     [741974]   \n","1        普通笔记  [http://sns-img-hw.xhscdn.com/3795c2fe-1819-34...     [739701]   \n","2        视频笔记  [http://sns-img-qc.xhscdn.com/6fd88377-9ffa-34...     [747652]   \n","3        视频笔记  [\"http://sns-img-qc.xhscdn.com/9f6a4597-1fd3-3...   [\"859906\"]   \n","4        视频笔记  [\"http://sns-img-qc.xhscdn.com/38e07503-6677-3...   [\"859912\"]   \n","\n","                                          author_url  site_id  fans_cnt  ...  \\\n","0  http://www.xiaohongshu.com/user/profile/5a2ff3...   146510    464770  ...   \n","1  http://www.xiaohongshu.com/user/profile/5a2ff3...   146510    457941  ...   \n","2  http://www.xiaohongshu.com/user/profile/5e8f86...   146510    555822  ...   \n","3  http://www.xiaohongshu.com/user/profile/565f83...   146510    535219  ...   \n","4  http://www.xiaohongshu.com/user/profile/55d6d2...   146510    856274  ...   \n","\n","      red_id video_cnt                   item_id follow_cnt coin_cnt  \\\n","0  420599937        94  60235cc900000000010282bf        185  2386973   \n","1  420599937        92  601e158f000000000101f856        185  2341527   \n","2    T654837       125  602b6729000000002103e7ff          7  2106984   \n","3  494694428       438  6023a0c7000000002103ecd0        111  2150628   \n","4   Qbaby414       416  601ab3fd000000000101f4b2          1  3099241   \n","\n","  data_type  user_location  entity_keywords.keyword  entity_keywords.type  \\\n","0      电商笔记         浙江  杭州                      NaN                   NaN   \n","1      电商笔记         浙江  杭州                      NaN                   NaN   \n","2      电商笔记         广西  南宁                      NaN                   NaN   \n","3      电商笔记             中国                      NaN                   NaN   \n","4      电商笔记         陕西  西安                      NaN                   NaN   \n","\n","  cooperation_brand_names  \n","0                     NaN  \n","1                     NaN  \n","2                     NaN  \n","3                     NaN  \n","4                     NaN  \n","\n","[5 rows x 49 columns]"],"text/html":["\n","  <div id=\"df-9f6708d2-6457-4822-9f9b-fdbf1bf65c15\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review_cnt</th>\n","      <th>is_hot</th>\n","      <th>publish_time</th>\n","      <th>id</th>\n","      <th>is_original</th>\n","      <th>pic_urls</th>\n","      <th>profilers_id</th>\n","      <th>author_url</th>\n","      <th>site_id</th>\n","      <th>fans_cnt</th>\n","      <th>...</th>\n","      <th>red_id</th>\n","      <th>video_cnt</th>\n","      <th>item_id</th>\n","      <th>follow_cnt</th>\n","      <th>coin_cnt</th>\n","      <th>data_type</th>\n","      <th>user_location</th>\n","      <th>entity_keywords.keyword</th>\n","      <th>entity_keywords.type</th>\n","      <th>cooperation_brand_names</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>84.0</td>\n","      <td>NaN</td>\n","      <td>2021-02-10 12:10:00</td>\n","      <td>60235cc900000000010282bf</td>\n","      <td>普通笔记</td>\n","      <td>[http://sns-img-qc.xhscdn.com/e12a7183-0d2b-3c...</td>\n","      <td>[741974]</td>\n","      <td>http://www.xiaohongshu.com/user/profile/5a2ff3...</td>\n","      <td>146510</td>\n","      <td>464770</td>\n","      <td>...</td>\n","      <td>420599937</td>\n","      <td>94</td>\n","      <td>60235cc900000000010282bf</td>\n","      <td>185</td>\n","      <td>2386973</td>\n","      <td>电商笔记</td>\n","      <td>浙江  杭州</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>76.0</td>\n","      <td>NaN</td>\n","      <td>2021-02-06 12:05:00</td>\n","      <td>601e158f000000000101f856</td>\n","      <td>普通笔记</td>\n","      <td>[http://sns-img-hw.xhscdn.com/3795c2fe-1819-34...</td>\n","      <td>[739701]</td>\n","      <td>http://www.xiaohongshu.com/user/profile/5a2ff3...</td>\n","      <td>146510</td>\n","      <td>457941</td>\n","      <td>...</td>\n","      <td>420599937</td>\n","      <td>92</td>\n","      <td>601e158f000000000101f856</td>\n","      <td>185</td>\n","      <td>2341527</td>\n","      <td>电商笔记</td>\n","      <td>浙江  杭州</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>爆文</td>\n","      <td>2021-02-16 14:33:00</td>\n","      <td>602b6729000000002103e7ff</td>\n","      <td>视频笔记</td>\n","      <td>[http://sns-img-qc.xhscdn.com/6fd88377-9ffa-34...</td>\n","      <td>[747652]</td>\n","      <td>http://www.xiaohongshu.com/user/profile/5e8f86...</td>\n","      <td>146510</td>\n","      <td>555822</td>\n","      <td>...</td>\n","      <td>T654837</td>\n","      <td>125</td>\n","      <td>602b6729000000002103e7ff</td>\n","      <td>7</td>\n","      <td>2106984</td>\n","      <td>电商笔记</td>\n","      <td>广西  南宁</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1182.0</td>\n","      <td>爆文</td>\n","      <td>2021-02-10 17:00:00</td>\n","      <td>6023a0c7000000002103ecd0</td>\n","      <td>视频笔记</td>\n","      <td>[\"http://sns-img-qc.xhscdn.com/9f6a4597-1fd3-3...</td>\n","      <td>[\"859906\"]</td>\n","      <td>http://www.xiaohongshu.com/user/profile/565f83...</td>\n","      <td>146510</td>\n","      <td>535219</td>\n","      <td>...</td>\n","      <td>494694428</td>\n","      <td>438</td>\n","      <td>6023a0c7000000002103ecd0</td>\n","      <td>111</td>\n","      <td>2150628</td>\n","      <td>电商笔记</td>\n","      <td>中国</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>523.0</td>\n","      <td>爆文</td>\n","      <td>2021-02-03 22:32:00</td>\n","      <td>601ab3fd000000000101f4b2</td>\n","      <td>视频笔记</td>\n","      <td>[\"http://sns-img-qc.xhscdn.com/38e07503-6677-3...</td>\n","      <td>[\"859912\"]</td>\n","      <td>http://www.xiaohongshu.com/user/profile/55d6d2...</td>\n","      <td>146510</td>\n","      <td>856274</td>\n","      <td>...</td>\n","      <td>Qbaby414</td>\n","      <td>416</td>\n","      <td>601ab3fd000000000101f4b2</td>\n","      <td>1</td>\n","      <td>3099241</td>\n","      <td>电商笔记</td>\n","      <td>陕西  西安</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 49 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f6708d2-6457-4822-9f9b-fdbf1bf65c15')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9f6708d2-6457-4822-9f9b-fdbf1bf65c15 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9f6708d2-6457-4822-9f9b-fdbf1bf65c15');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["sample = df['content'].sample(n=10000, random_state=1)\n","sample.to_excel(\"/content/randomsample.xlsx\")"],"metadata":{"id":"J5D_fG6mlIcY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Mount Drive for saving"],"metadata":{"id":"BfcDHbR5iE77"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lu9Cd8DaiCSv","executionInfo":{"status":"ok","timestamp":1689493862355,"user_tz":-480,"elapsed":1972,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}},"outputId":"0ae9c1d0-f8ee-47c2-ea11-ff0931953459"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["# Data Preprocessing"],"metadata":{"id":"O4jNp3mDPYZH"}},{"cell_type":"code","source":["!pip install jieba"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CQBDRkNkxmsy","executionInfo":{"status":"ok","timestamp":1689493871709,"user_tz":-480,"elapsed":4521,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}},"outputId":"a3393e6f-19c1-4c93-9fd9-3e541d17fb34"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (0.42.1)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip install -U pip setuptools wheel\n","!pip install -U spacy\n","!python -m spacy download zh_core_web_trf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UXOmiLYF2eU_","executionInfo":{"status":"ok","timestamp":1689493924424,"user_tz":-480,"elapsed":50294,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}},"outputId":"de892e0b-d53f-407f-bc8a-fc561ca0afb6"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (68.0.0)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.40.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.10)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.11)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (68.0.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.0)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m2023-07-16 07:51:24.779686: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-07-16 07:51:24.834022: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-07-16 07:51:25.897537: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Collecting zh-core-web-trf==3.6.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/zh_core_web_trf-3.6.1/zh_core_web_trf-3.6.1-py3-none-any.whl (417.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.4/417.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from zh-core-web-trf==3.6.1) (3.6.0)\n","Requirement already satisfied: spacy-transformers<1.3.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from zh-core-web-trf==3.6.1) (1.2.5)\n","Requirement already satisfied: spacy-pkuseg<0.1.0,>=0.0.27 in /usr/local/lib/python3.10/dist-packages (from zh-core-web-trf==3.6.1) (0.0.32)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (1.0.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (1.0.9)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (3.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (8.1.10)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (2.4.6)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (2.0.8)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (0.10.2)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (6.3.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (4.65.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (1.22.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (2.27.1)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (1.10.11)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (68.0.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (23.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (3.3.0)\n","Requirement already satisfied: transformers<4.31.0,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (4.30.2)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (2.0.1+cu118)\n","Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (0.9.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (4.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (3.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (0.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (3.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (3.1)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (16.0.6)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (0.16.4)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (2022.10.31)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (0.3.1)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (8.1.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (2.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (2023.6.0)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (1.3.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('zh_core_web_trf')\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import jieba\n","import jieba.analyse\n","from nltk.corpus import stopwords\n","import numpy as np"],"metadata":{"id":"g3jxI-N6U3aK","executionInfo":{"status":"ok","timestamp":1689493925529,"user_tz":-480,"elapsed":1116,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def stopwordslist():\n","        stopwords = [line.strip() for line in open('/content/drive/MyDrive/Dissertation/stopwords.txt',encoding='utf-8').readlines()]\n","        return stopwords\n","\n","def seg_depart(sentence):\n","    sentence_depart = jieba.lcut(sentence)\n","    stopwords = stopwordslist()\n","    outstr=''\n","    for word in sentence_depart:\n","        if word not in stopwords:\n","            outstr += word\n","            outstr += ' '\n","    return outstr"],"metadata":{"id":"IAZOZ9uVDjqp","executionInfo":{"status":"ok","timestamp":1689493925530,"user_tz":-480,"elapsed":7,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Text Segmentation of Train set (without label)"],"metadata":{"id":"CJCKrgpwevm5"}},{"cell_type":"code","source":["# Split into testing set and training without label set and output as txt file\n","\n","from sklearn.model_selection import train_test_split\n","dt = pd.read_excel('/content/drive/MyDrive/Dissertation/Original.xlsx')\n","X = dt['content']\n","\n","train, test = train_test_split(X, test_size=0.005, random_state=2)\n","\n","#test.to_excel(\"/content/drive/MyDrive/Dissertation/test_verification.xlsx\")"],"metadata":{"id":"1LQCIT4QfR0M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_train = []\n","\n","for line in train.astype(str):\n","  line_seg=seg_depart(line)\n","  output_train.append(line_seg)\n","print('success！') # output_train is the segmented list for training(without label) set"],"metadata":{"id":"aZGn9fkbee-j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689492671452,"user_tz":-480,"elapsed":509204,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}},"outputId":"712b8748-b217-44a2-a4a9-cba41f3cd1df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Building prefix dict from the default dictionary ...\n","DEBUG:jieba:Building prefix dict from the default dictionary ...\n","Dumping model to file cache /tmp/jieba.cache\n","DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n","Loading model cost 0.694 seconds.\n","DEBUG:jieba:Loading model cost 0.694 seconds.\n","Prefix dict has been built successfully.\n","DEBUG:jieba:Prefix dict has been built successfully.\n"]},{"output_type":"stream","name":"stdout","text":["success！\n"]}]},{"cell_type":"code","source":["file = open('/content/drive/MyDrive/Dissertation/Train_nolabel.txt','w')\n","for item in output_train:\n","\tfile.write(item+\"\\n\")\n","file.close()\n","print('success')"],"metadata":{"id":"fyzo6rz12bz1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689492673872,"user_tz":-480,"elapsed":2423,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}},"outputId":"df83d70f-8b09-43b8-c8e2-fd893bc55c89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["success\n"]}]},{"cell_type":"markdown","source":["# Text Segmentation of Testing set (without label)"],"metadata":{"id":"jnkcyCzd3Jf4"}},{"cell_type":"code","source":["output_test = []\n","\n","for line in test.astype(str):\n","  line_seg=seg_depart(line)\n","  output_test.append(line_seg)\n","print('success！') # output_test is the segmented list for testing set"],"metadata":{"id":"ud5Ikvqd3BoK","colab":{"base_uri":"https://localhost:8080/","height":239},"executionInfo":{"status":"error","timestamp":1689494483668,"user_tz":-480,"elapsed":476,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}},"outputId":"1eee2ced-913a-497a-d36b-535e83576c93"},"execution_count":23,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-d8758046b4c1>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutput_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mline_seg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseg_depart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0moutput_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_seg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"]}]},{"cell_type":"code","source":["file = open('/content/drive/MyDrive/Dissertation/Test.txt','w')\n","for item in output_test:\n","\tfile.write(item+\"\\n\")\n","file.close()\n","print('success！')"],"metadata":{"id":"Zsl5lKUl3C0P","executionInfo":{"status":"aborted","timestamp":1689494483669,"user_tz":-480,"elapsed":4,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Text Segmentation of Training Set (with label)"],"metadata":{"id":"EeFLgiWkB7tf"}},{"cell_type":"code","source":["dr = pd.read_excel('/content/drive/MyDrive/Dissertation/randomsample.xlsx', index_col='index')\n","#content = df['content'].tolist()\n","\n","dr1 = dr.loc[dr['label'] == 1]\n","dr0 = dr.loc[dr['label'] == 0]\n","\n","print('Sample size before downsize: '+str(len(dr)))\n","print('Positive sample size before downsize: '+str(len(dr1)))\n","print('Negative sample size before downsize: '+str(len(dr0)))\n","\n","dr0 = dr0.sample(frac=1, replace=True, random_state=1) #downsize the major group\n","# 0.015 resulted 15 positive sample\n","\n","\n","df = pd.concat([dr1,dr0], ignore_index=True, sort=False)\n","\n","df = df.sample(frac=1).reset_index() #shuffle the order of the trainning sample\n","\n","label = df['label'].tolist()\n","\n","print('Sample size after downsize: '+str(len(df)))\n","print('Positive sample size after downsize: '+str(len(df.loc[df['label']== 1])))\n","print('Negative sample size after downsize: '+str(len(df.loc[df['label']== 0])))\n","print('Proportion for Positive sample after downsize: '+str(len(df.loc[df['label']== 1])/len(df)))"],"metadata":{"id":"-6jEjP3SuCgy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689494561355,"user_tz":-480,"elapsed":684,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}},"outputId":"95f72736-7297-4ce5-ee9a-f5db618b670b"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample size before downsize: 10000\n","Positive sample size before downsize: 879\n","Negative sample size before downsize: 9121\n","Sample size after downsize: 10000\n","Positive sample size after downsize: 879\n","Negative sample size after downsize: 9121\n","Proportion for Positive sample after downsize: 0.0879\n"]}]},{"cell_type":"code","source":["output = []\n","\n","for line in df['content'].astype(str):\n","  line_seg=seg_depart(line)\n","  output.append(line_seg) # output is the segmented list for training(with label) set\n","print('success！')"],"metadata":{"id":"SKeXmaX2cHcX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689494604264,"user_tz":-480,"elapsed":39208,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}},"outputId":"347f824a-f307-4831-e8f3-8f63b43e59b8"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["success！\n"]}]},{"cell_type":"code","source":["ele = [\" +++$+++ \"]\n","element = ele * len(df) # add separator element to separate label and content\n","#print(element)\n","\n","concat_function = lambda x,y: str(x) + \"\" + y # change label as string and concat separator element\n","concat_func = lambda x,y: x + \"\" + y # concat label and training set\n","\n","a = list(map(concat_function,label,element))\n","\n","labels = list(map(concat_func,a,output))"],"metadata":{"id":"2I9nu3JQ03kg","executionInfo":{"status":"ok","timestamp":1689494659921,"user_tz":-480,"elapsed":7,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# output as txt file Train with label\n","\n","file = open('/content/drive/MyDrive/Dissertation/Train_label.txt','w')\n","for item in labels:\n","\tfile.write(item+\"\\n\")\n","file.close()\n","\n","print('success！')"],"metadata":{"id":"-ShBP7Rkef3k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689494662720,"user_tz":-480,"elapsed":8,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}},"outputId":"c5c5383d-4e3c-4178-85f8-043f2b0b3659"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["success！\n"]}]},{"cell_type":"markdown","source":["# Model Training"],"metadata":{"id":"z-1Abhci7qxi"}},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')\n","path_prefix = './'"],"metadata":{"id":"4JUeQbbf7yNR","executionInfo":{"status":"ok","timestamp":1689493955694,"user_tz":-480,"elapsed":635,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import pandas as pd\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","def load_training_data(path='/content/drive/MyDrive/Dissertation/Train_label.txt'):\n","    # Read training data\n","    if 'Train_label' in path:\n","        with open(path, 'r', encoding='utf-8') as f:\n","            lines = f.readlines()\n","            lines = [line.strip('\\n').split(' ') for line in lines]\n","        x = [line[2:] for line in lines]\n","        y = [line[0] for line in lines]\n","        return x, y\n","    else:\n","        with open(path, 'r', encoding='utf-8') as f:\n","            lines = f.readlines()\n","            x = [line.strip('\\n').split(' ') for line in lines]\n","        return x\n","\n","def load_testing_data(path='/content/drive/MyDrive/Dissertation/Test.txt'):\n","    # Read testing data\n","    with open(path, 'r', encoding='utf-8') as f:\n","        lines = f.readlines()\n","        X = [\"\".join(line.strip('\\n').split(\",\")).strip() for line in lines]\n","        X = [sen.split(' ') for sen in X]\n","    return X\n","\n","def evaluation(outputs, labels):\n","    #outputs => probability (float)\n","    #labels => labels\n","    outputs[outputs>=0.5] = 1 # Negtive Sentiment\n","    outputs[outputs<0.5] = 0 # Positive Sentiment\n","    correct = torch.sum(torch.eq(outputs, labels)).item()\n","    return correct"],"metadata":{"id":"LUzQlX5x799e","executionInfo":{"status":"ok","timestamp":1689493963110,"user_tz":-480,"elapsed":1972,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["# Train Word to Vector"],"metadata":{"id":"3Xj-En_J8E6E"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import argparse\n","from gensim.models import word2vec\n","\n","def train_word2vec(x):\n","    model = word2vec.Word2Vec(x, vector_size=250, window=5, min_count=5, workers=12)\n","    return model\n","\n","if __name__ == \"__main__\":\n","    print(\"loading training data ...\")\n","    train_x, y = load_training_data('/content/drive/MyDrive/Dissertation/Train_label.txt')\n","    train_x_no_label = load_training_data('/content/drive/MyDrive/Dissertation/Train_nolabel.txt')\n","\n","    print(\"loading testing data ...\")\n","    test_x = load_testing_data('/content/drive/MyDrive/Dissertation/Test.txt')\n","\n","    model = train_word2vec(train_x + train_x_no_label + test_x)\n","\n","    print(\"saving model ...\")\n","    model.save(os.path.join(path_prefix, '/w2v_all.model'))"],"metadata":{"id":"bnfXyCOh8A0G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689494031676,"user_tz":-480,"elapsed":48966,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}},"outputId":"73389cec-1fc7-4e55-e2fd-5c3f5c0cc7fd"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["loading training data ...\n","loading testing data ...\n","saving model ...\n"]}]},{"cell_type":"markdown","source":["# Data Preprocess"],"metadata":{"id":"KfFTVTll8Xt6"}},{"cell_type":"code","source":["from torch import nn\n","from gensim.models import Word2Vec\n","\n","class Preprocess():\n","    def __init__(self, sentences, sen_len, w2v_path=\"./w2v.model\"):\n","        self.w2v_path = w2v_path\n","        self.sentences = sentences\n","        self.sen_len = sen_len\n","        self.idx2word = []\n","        self.word2idx = {}\n","        self.embedding_matrix = []\n","    def get_w2v_model(self):\n","        # load word to vector model\n","        self.embedding = Word2Vec.load(self.w2v_path)\n","        self.embedding_dim = self.embedding.vector_size\n","    def add_embedding(self, word):\n","        # add word into embedding\n","        vector = torch.empty(1, self.embedding_dim)\n","        torch.nn.init.uniform_(vector)\n","        self.word2idx[word] = len(self.word2idx)\n","        self.idx2word.append(word)\n","        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)\n","    def make_embedding(self, load=True):\n","        print(\"Get embedding ...\")\n","        if load:\n","            print(\"loading word to vec model ...\")\n","            self.get_w2v_model()\n","        else:\n","            raise NotImplementedError\n","\n","        for i, word in enumerate(self.embedding.wv.key_to_index):\n","            print('get words #{}'.format(i+1), end='\\r')\n","            self.word2idx[word] = len(self.word2idx)\n","            self.idx2word.append(word)\n","            self.embedding_matrix.append(self.embedding.wv[word])\n","        print('')\n","        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n","        self.add_embedding(\"<PAD>\")\n","        self.add_embedding(\"<UNK>\")\n","        print(\"total words: {}\".format(len(self.embedding_matrix)))\n","        return self.embedding_matrix\n","    def pad_sequence(self, sentence):\n","        if len(sentence) > self.sen_len:\n","            sentence = sentence[:self.sen_len]\n","        else:\n","            pad_len = self.sen_len - len(sentence)\n","            for _ in range(pad_len):\n","                sentence.append(self.word2idx[\"<PAD>\"])\n","        assert len(sentence) == self.sen_len\n","        return sentence\n","    def sentence_word2idx(self):\n","        sentence_list = []\n","        for i, sen in enumerate(self.sentences):\n","            print('sentence count #{}'.format(i+1), end='\\r')\n","            sentence_idx = []\n","            for word in sen:\n","                if (word in self.word2idx.keys()):\n","                    sentence_idx.append(self.word2idx[word])\n","                else:\n","                    sentence_idx.append(self.word2idx[\"<UNK>\"])\n","            sentence_idx = self.pad_sequence(sentence_idx)\n","            sentence_list.append(sentence_idx)\n","        return torch.LongTensor(sentence_list)\n","    def labels_to_tensor(self, y):\n","        # turn labels into tensors\n","        y = [int(label) for label in y]\n","        return torch.LongTensor(y)"],"metadata":{"id":"xECUJfRy8VhO","executionInfo":{"status":"ok","timestamp":1689494058630,"user_tz":-480,"elapsed":493,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["# Dataset"],"metadata":{"id":"YN0qajNL8ejJ"}},{"cell_type":"code","source":["import torch\n","from torch.utils import data\n","\n","class TwitterDataset(data.Dataset):\n","    \"\"\"\n","    Expected data shape like:(data_num, data_len)\n","    Data can be a list of numpy array or a list of lists\n","    input data shape : (data_num, seq_len, feature_dim)\n","\n","    __len__ will return the number of data\n","    \"\"\"\n","    def __init__(self, X, y):\n","        self.data = X\n","        self.label = y\n","    def __getitem__(self, idx):\n","        if self.label is None: return self.data[idx]\n","        return self.data[idx], self.label[idx]\n","    def __len__(self):\n","        return len(self.data)"],"metadata":{"id":"pidDfJOu8dii","executionInfo":{"status":"ok","timestamp":1689494062787,"user_tz":-480,"elapsed":1161,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# LSTM Model"],"metadata":{"id":"Y5QzeI6i8kFQ"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","class LSTM_Net(nn.Module):\n","    def __init__(self, embedding, embedding_dim, hidden_dim, num_layers, dropout=0.5, fix_embedding=True):\n","        super(LSTM_Net, self).__init__()\n","        # embedding layer\n","        self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))\n","        self.embedding.weight = torch.nn.Parameter(embedding)\n","        # Whether fix embedding\n","        self.embedding.weight.requires_grad = False if fix_embedding else True\n","        self.embedding_dim = embedding.size(1)\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","        self.dropout = dropout\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n","        self.classifier = nn.Sequential( nn.Dropout(dropout),\n","                          nn.Linear(hidden_dim, 1),\n","                          nn.Sigmoid() )\n","    def forward(self, inputs):\n","        inputs = self.embedding(inputs)\n","        x, _ = self.lstm(inputs, None)\n","        # dimension of x (batch, seq_len, hidden_size)\n","        x = x[:, -1, :]\n","        x = self.classifier(x)\n","        return x"],"metadata":{"id":"c-0IPClf8nyY","executionInfo":{"status":"ok","timestamp":1689494068085,"user_tz":-480,"elapsed":453,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["# Define Training"],"metadata":{"id":"RoKr7ggM8s6a"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","def training(batch_size, n_epoch, lr, model_dir, train, valid, model, device):\n","    total = sum(p.numel() for p in model.parameters())\n","    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print('\\nstart training, parameter total:{}, trainable:{}\\n'.format(total, trainable))\n","    model.train() # set training mode\n","    criterion = nn.BCELoss() # Define loss function\n","    t_batch = len(train)\n","    v_batch = len(valid)\n","    optimizer = optim.Adam(model.parameters(), lr=lr) # set optimizer as Adam\n","    total_loss, total_acc, best_acc = 0, 0, 0\n","    epoch_num=range(1,6)#1,2,3,4,5\n","    train_loss=[]\n","    valid_loss=[]\n","    for epoch in range(n_epoch):\n","        total_loss, total_acc = 0, 0\n","\n","        # For training\n","        model.train() #set training mode\n","        for i, (inputs, labels) in enumerate(train):\n","            inputs = inputs.to(device, dtype=torch.long) # set device \"cuda\"\n","            labels = labels.to(device, dtype=torch.float) # set device \"cuda\"\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            outputs = outputs.squeeze()\n","            model.train()\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            correct = evaluation(outputs, labels) # calculate accuracy\n","            total_acc += (correct / batch_size)\n","            total_loss += loss.item()\n","            print('[ Epoch{}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(\n","            \tepoch+1, i+1, t_batch, loss.item(), correct*100/batch_size), end='\\r')\n","        print('\\nTrain | Loss:{:.5f} Acc: {:.3f}'.format(total_loss/t_batch, total_acc/t_batch*100))\n","        train_loss.append(total_loss/t_batch)\n","\n","        # For validation\n","        model.eval() # set validation mode\n","        with torch.no_grad():\n","            total_loss, total_acc = 0, 0\n","            for i, (inputs, labels) in enumerate(valid):\n","                inputs = inputs.to(device, dtype=torch.long) # set device \"cuda\"\n","                labels = labels.to(device, dtype=torch.float) # set device \"cuda\"\n","                outputs = model(inputs)\n","                outputs = outputs.squeeze()\n","                loss = criterion(outputs, labels)\n","                correct = evaluation(outputs, labels)\n","                total_acc += (correct / batch_size)\n","                total_loss += loss.item()\n","\n","            print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss/v_batch, total_acc/v_batch*100))\n","            if total_acc > best_acc:\n","                # if the result of validation is better than previous model, save the new model\n","                best_acc = total_acc\n","                torch.save(model, \"{}/ckpt.model\".format(model_dir))\n","                print('saving model with acc {:.3f}'.format(total_acc/v_batch*100))\n","        print('-----------------------------------------------')\n","        valid_loss.append(total_loss/v_batch)"],"metadata":{"id":"VxpLcHOW8rjA","executionInfo":{"status":"ok","timestamp":1689494074799,"user_tz":-480,"elapsed":452,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["# Define Pesudo Labelling"],"metadata":{"id":"HaxC3Y0a-NCV"}},{"cell_type":"code","source":["\n","import torch\n","from torch import nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import math\n","\n","def got_label(batch_size, test_loader, model, device):\n","    model.eval()\n","    ret_output = []\n","    with torch.no_grad():\n","        for i, inputs in enumerate(test_loader):\n","            inputs = inputs.to(device, dtype=torch.long)\n","            outputs = model(inputs)\n","            outputs = outputs.squeeze()\n","            #outputs[outputs>=0.9] = 1 # cannot change into 1 first\n","            #outputs[outputs<0.1] = 0 # cannot change into 0 first\n","\n","            ret_output += outputs.tolist() #cannot use int() as we need float\n","\n","    return ret_output #pesudo_x, pesudo_y\n"],"metadata":{"id":"X6MW6gcr-NrW","executionInfo":{"status":"ok","timestamp":1689494080511,"user_tz":-480,"elapsed":2,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["# Testing"],"metadata":{"id":"jH-w6UfJ8xWg"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","def testing(batch_size, test_loader, model, device):\n","    model.eval()\n","    ret_output = []\n","    with torch.no_grad():\n","        for i, inputs in enumerate(test_loader):\n","            inputs = inputs.to(device, dtype=torch.long)\n","            outputs = model(inputs)\n","            outputs = outputs.squeeze()\n","            outputs[outputs>=0.5] = 1\n","            outputs[outputs<0.5] = 0\n","            #print(outputs)\n","            ret_output += outputs.int().tolist()\n","\n","    return ret_output"],"metadata":{"id":"iIIxhLO980G9","executionInfo":{"status":"ok","timestamp":1689494087321,"user_tz":-480,"elapsed":454,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["# Parameter setting + Train"],"metadata":{"id":"al-yG_kp84bZ"}},{"cell_type":"code","source":["# main.py\n","import os\n","import torch\n","import argparse\n","import numpy as np\n","from torch import nn\n","from gensim.models import word2vec\n","from sklearn.model_selection import train_test_split\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# set data path\n","train_with_label = os.path.join(path_prefix, '/content/drive/MyDrive/Dissertation/Train_label.txt')\n","train_no_label = os.path.join(path_prefix, '/content/drive/MyDrive/Dissertation/Train_nolabel.txt')\n","testing_data = os.path.join(path_prefix, '/content/drive/MyDrive/Dissertation/Test.txt')\n","w2v_path = os.path.join(path_prefix, '/w2v_all.model')\n","\n","\n","sen_len = 250\n","fix_embedding = True # fix embedding during training\n","batch_size = 96\n","#batch_size = 32\n","#batch_size = 64\n","#epoch = 2\n","epoch = 30\n","lr = 0.00005 # increase learning rate\n","model_dir = path_prefix\n","\n","print(\"loading data ...\")\n","train_x, y = load_training_data(train_with_label)\n","train_x_no_label = load_training_data(train_no_label)\n","\n","# Preprocessing\n","preprocess = Preprocess(train_x, sen_len, w2v_path=w2v_path)\n","embedding = preprocess.make_embedding(load=True)\n","train_x = preprocess.sentence_word2idx()\n","y = preprocess.labels_to_tensor(y)\n","\n","\n","model = LSTM_Net(embedding, embedding_dim=250, hidden_dim=125, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\n","model = model.to(device)\n","\n","#X_train, X_val, y_train, y_val = train_x[:130000], train_x[130000:], y[:130000], y[130000:]\n","\n","X_train, X_val, y_train, y_val = train_test_split(train_x, y, test_size = 0.1, random_state = 1, stratify=y)\n","\n","train_dataset = TwitterDataset(X=X_train, y=y_train)\n","val_dataset = TwitterDataset(X=X_val, y=y_val)\n","\n","# transfor data into batch of tensors\n","train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n","                             batch_size = batch_size,\n","                             shuffle = True,\n","                             num_workers = 8)\n","\n","val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n","                           batch_size = batch_size,\n","                           shuffle = False,\n","                           num_workers = 8)\n","\n","#train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n","#                            batch_size = batch_size,\n","#                            shuffle = True,\n","#                            num_workers = 0)\n","\n","#val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n","#                            batch_size = batch_size,\n","#                           shuffle = False,\n","#                           num_workers = 0)\n","\n","# Begin Training\n","training(batch_size, epoch, lr, model_dir, train_loader, val_loader, model, device)\n"],"metadata":{"id":"qTOKtIjT88FL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"60a21823-e117-4539-e248-d72084c86cae","executionInfo":{"status":"ok","timestamp":1689495194666,"user_tz":-480,"elapsed":88502,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}}},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["loading data ...\n","Get embedding ...\n","loading word to vec model ...\n","get words #73201\n","total words: 73203\n","\n","start training, parameter total:18489376, trainable:188626\n","\n","\n","Train | Loss:0.43243 Acc: 88.475\n","Valid | Loss:0.31570 Acc: 85.701 \n","saving model with acc 85.701\n","-----------------------------------------------\n","\n","Train | Loss:0.30984 Acc: 90.215\n","Valid | Loss:0.30227 Acc: 86.269 \n","saving model with acc 86.269\n","-----------------------------------------------\n","\n","Train | Loss:0.30282 Acc: 90.714\n","Valid | Loss:0.29668 Acc: 86.364 \n","saving model with acc 86.364\n","-----------------------------------------------\n","\n","Train | Loss:0.29910 Acc: 90.880\n","Valid | Loss:0.29287 Acc: 86.364 \n","-----------------------------------------------\n","\n","Train | Loss:0.29622 Acc: 90.924\n","Valid | Loss:0.29063 Acc: 86.364 \n","-----------------------------------------------\n","\n","Train | Loss:0.29467 Acc: 90.924\n","Valid | Loss:0.28921 Acc: 86.364 \n","-----------------------------------------------\n","\n","Train | Loss:0.29399 Acc: 90.969\n","Valid | Loss:0.28842 Acc: 86.364 \n","-----------------------------------------------\n","\n","Train | Loss:0.29465 Acc: 90.969\n","Valid | Loss:0.28760 Acc: 86.364 \n","-----------------------------------------------\n","\n","Train | Loss:0.29311 Acc: 90.946\n","Valid | Loss:0.28715 Acc: 86.364 \n","-----------------------------------------------\n","\n","Train | Loss:0.29009 Acc: 90.980\n","Valid | Loss:0.28686 Acc: 86.364 \n","-----------------------------------------------\n","[ Epoch11: 94/94 ] loss:0.334 acc:67.708 \n","Train | Loss:0.28946 Acc: 90.980\n","Valid | Loss:0.28636 Acc: 86.364 \n","-----------------------------------------------\n","\n","Train | Loss:0.28936 Acc: 90.969\n","Valid | Loss:0.28614 Acc: 86.364 \n","-----------------------------------------------\n","\n","Train | Loss:0.28917 Acc: 90.969\n","Valid | Loss:0.28563 Acc: 86.364 \n","-----------------------------------------------\n","\n","Train | Loss:0.28869 Acc: 90.991\n","Valid | Loss:0.28514 Acc: 86.364 \n","-----------------------------------------------\n","\n","Train | Loss:0.28526 Acc: 91.002\n","Valid | Loss:0.28430 Acc: 86.364 \n","-----------------------------------------------\n","\n","Train | Loss:0.28383 Acc: 91.024\n","Valid | Loss:0.28230 Acc: 86.364 \n","-----------------------------------------------\n","\n","Train | Loss:0.28206 Acc: 91.079\n","Valid | Loss:0.28121 Acc: 86.458 \n","saving model with acc 86.458\n","-----------------------------------------------\n","\n","Train | Loss:0.28116 Acc: 91.102\n","Valid | Loss:0.27978 Acc: 86.458 \n","-----------------------------------------------\n","\n","Train | Loss:0.28068 Acc: 91.212\n","Valid | Loss:0.28298 Acc: 86.458 \n","-----------------------------------------------\n","[ Epoch20: 94/94 ] loss:0.338 acc:67.708 \n","Train | Loss:0.27990 Acc: 91.201\n","Valid | Loss:0.28354 Acc: 86.553 \n","saving model with acc 86.553\n","-----------------------------------------------\n","\n","Train | Loss:0.27776 Acc: 91.312\n","Valid | Loss:0.28100 Acc: 86.458 \n","-----------------------------------------------\n","\n","Train | Loss:0.27887 Acc: 91.379\n","Valid | Loss:0.28965 Acc: 85.322 \n","-----------------------------------------------\n","\n","Train | Loss:0.27901 Acc: 91.367\n","Valid | Loss:0.27885 Acc: 86.458 \n","-----------------------------------------------\n","\n","Train | Loss:0.27590 Acc: 91.467\n","Valid | Loss:0.27962 Acc: 86.269 \n","-----------------------------------------------\n","\n","Train | Loss:0.27503 Acc: 91.445\n","Valid | Loss:0.27873 Acc: 86.080 \n","-----------------------------------------------\n","\n","Train | Loss:0.27606 Acc: 91.500\n","Valid | Loss:0.28085 Acc: 86.458 \n","-----------------------------------------------\n","\n","Train | Loss:0.27456 Acc: 91.500\n","Valid | Loss:0.28284 Acc: 86.458 \n","-----------------------------------------------\n","\n","Train | Loss:0.27349 Acc: 91.445\n","Valid | Loss:0.27751 Acc: 86.364 \n","-----------------------------------------------\n","\n","Train | Loss:0.27425 Acc: 91.567\n","Valid | Loss:0.28288 Acc: 86.458 \n","-----------------------------------------------\n","\n","Train | Loss:0.27312 Acc: 91.622\n","Valid | Loss:0.28265 Acc: 86.458 \n","-----------------------------------------------\n"]}]},{"cell_type":"code","source":["### Get pesudo label\n","train_x_no_label_word = load_training_data(train_no_label)\n","train_x_no_label = preprocess.sentence_word2idx()\n","preprocess = Preprocess(train_x_no_label, sen_len, w2v_path=w2v_path)\n","embedding = preprocess.make_embedding(load=True)\n","train_x_no_label_dataset = TwitterDataset(X=train_x_no_label, y=None)\n","\n","train_no_loader = torch.utils.data.DataLoader(dataset = train_x_no_label_dataset,\n","                            batch_size = batch_size,\n","                            shuffle = False,\n","                            num_workers = 0)\n","\n","model = torch.load(os.path.join(model_dir, 'ckpt.model'))\n","output = got_label(batch_size, train_no_loader, model, device)\n"],"metadata":{"id":"7uGKqXED-cdf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689495253827,"user_tz":-480,"elapsed":26770,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}},"outputId":"aeadda5e-d54f-478f-c2ad-b19aa8f8ff55"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Get embedding ...\n","loading word to vec model ...\n","get words #73201\n","total words: 73203\n"]}]},{"cell_type":"code","source":["\n","### Pesudo labelled sample selection\n","#import Tensor\n","\n","pesudo_label_index = []\n","\n","#pesudo_x = preprocess.sentence_word2idx()\n","#preprocess = Preprocess(pesudo_x, sen_len, w2v_path=w2v_path)\n","#embedding = preprocess.make_embedding(load=True)\n","\n","for i in range(len(output)):\n","  if output[i] >= 0.9 or output[i] <= 0.1:\n","  #if output[i] >= 0.8:\n","    pesudo_label_index.append(i)\n","\n","pesudo_x = [train_x_no_label[i] for i in pesudo_label_index]\n","pesudo_y = [output[i] for i in pesudo_label_index]\n","\n","for i in range(len(pesudo_y)):\n","  if pesudo_y[i] <= 0.3:\n","    pesudo_y[i] = 0.0\n","  else:\n","    pesudo_y[i] = 1.0\n","\n","#print(pesudo_x)\n","\n","#pesudo_x = TwitterDataset(pesudo_x,  y=None)\n","#pesudo_x = torch.Tensor(pesudo_x)\n"],"metadata":{"id":"aw0-VD38-fAD","executionInfo":{"status":"ok","timestamp":1689495263638,"user_tz":-480,"elapsed":498,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["print(\"loading data ...\")\n","train_x, y = load_training_data(train_with_label)\n","preprocess = Preprocess(train_x, sen_len, w2v_path=w2v_path)\n","embedding = preprocess.make_embedding(load=True)\n","train_x = preprocess.sentence_word2idx()\n","y = preprocess.labels_to_tensor(y)\n","\n","model = LSTM_Net(embedding, embedding_dim=250, hidden_dim=125, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\n","model = model.to(device)\n","\n","X_train, X_val, y_train, y_val = train_test_split(train_x, y, test_size = 0.05, random_state = 10, stratify=y)\n","\n","# Combine labelled samples with pesudo labelled samples\n","\n","pesudo_x = torch.tensor([item.cpu().detach().numpy() for item in pesudo_x]).cuda() #turn list to tensor\n","pesudo_x = pesudo_x.to(device, dtype=torch.long) # set device \"cuda\"\n","X_train = X_train.to(device, dtype=torch.long) # turn list to tensor\n","retrain_x = torch.cat((X_train,pesudo_x),0)  # combine with label and without label dataset into one\n","pesudo_y = preprocess.labels_to_tensor(pesudo_y) #turn list to tensor\n","pesudo_y = torch.Tensor(pesudo_y) # turn list to tensor\n","pesudo_y = pesudo_y.to(device, dtype=torch.float) # set device \"cuda\"\n","y_train = y_train.to(device, dtype=torch.float) # set device \"cuda\"\n","retrain_y = torch.cat((y_train,pesudo_y),0)  # combine with label and without label dataset into one\n"],"metadata":{"id":"6Uxv1sU9-hH8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689495291765,"user_tz":-480,"elapsed":23958,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}},"outputId":"36098aef-2d16-4fef-a39a-8e3243206933"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["loading data ...\n","Get embedding ...\n","loading word to vec model ...\n","get words #73201\n","total words: 73203\n"]}]},{"cell_type":"code","source":["#print(pesudo_y)"],"metadata":{"id":"mhTpdKPjPv-w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Retrain model with pesudo labelled samples\n","#batch_size = 192\n","batch_size = 96\n","#batch_size = 32\n","#epoch = 2\n","#epoch = 5\n","epoch = 50\n","#epoch = 7\n","lr = 0.00005 # use a smaller learning rate\n","\n","retrain_dataset = TwitterDataset(X=retrain_x, y=retrain_y)\n","val_dataset = TwitterDataset(X=X_val, y=y_val)\n","\n","# transfor data into batch of tensors\n","retrain_loader = torch.utils.data.DataLoader(dataset = retrain_dataset,\n","                          batch_size = batch_size,\n","                          shuffle = True,\n","                          num_workers = 0)\n","\n","val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n","                      batch_size = batch_size,\n","                      shuffle = False,\n","                      num_workers = 0)\n","\n","training(batch_size, epoch, lr, model_dir, retrain_loader, val_loader, model, device)\n"],"metadata":{"id":"sj9xnNsl-jAf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689495344886,"user_tz":-480,"elapsed":46055,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}},"outputId":"ff5e3b1f-fd2c-4c89-dc10-ccd86fa447ae"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","start training, parameter total:18489376, trainable:188626\n","\n","[ Epoch1: 196/196 ] loss:0.165 acc:41.667 \n","Train | Loss:0.29871 Acc: 90.290\n","Valid | Loss:0.32470 Acc: 78.993 \n","saving model with acc 78.993\n","-----------------------------------------------\n","[ Epoch2: 196/196 ] loss:0.271 acc:39.583 \n","Train | Loss:0.18912 Acc: 95.052\n","Valid | Loss:0.32137 Acc: 78.993 \n","saving model with acc 78.993\n","-----------------------------------------------\n","[ Epoch3: 196/196 ] loss:0.134 acc:41.667 \n","Train | Loss:0.17952 Acc: 95.227\n","Valid | Loss:0.32005 Acc: 79.167 \n","saving model with acc 79.167\n","-----------------------------------------------\n","[ Epoch4: 196/196 ] loss:0.130 acc:41.667 \n","Train | Loss:0.17748 Acc: 95.270\n","Valid | Loss:0.31947 Acc: 79.167 \n","-----------------------------------------------\n","[ Epoch5: 196/196 ] loss:0.198 acc:40.625 \n","Train | Loss:0.17689 Acc: 95.270\n","Valid | Loss:0.31567 Acc: 79.167 \n","-----------------------------------------------\n","[ Epoch6: 196/196 ] loss:0.098 acc:41.667 \n","Train | Loss:0.17568 Acc: 95.275\n","Valid | Loss:0.31511 Acc: 79.167 \n","-----------------------------------------------\n","[ Epoch7: 196/196 ] loss:0.170 acc:40.625 \n","Train | Loss:0.17490 Acc: 95.275\n","Valid | Loss:0.31658 Acc: 79.167 \n","-----------------------------------------------\n","[ Epoch8: 196/196 ] loss:0.309 acc:39.583 \n","Train | Loss:0.17358 Acc: 95.270\n","Valid | Loss:0.31679 Acc: 79.167 \n","-----------------------------------------------\n","[ Epoch9: 196/196 ] loss:0.289 acc:38.542 \n","Train | Loss:0.17258 Acc: 95.302\n","Valid | Loss:0.30655 Acc: 79.167 \n","-----------------------------------------------\n","[ Epoch10: 196/196 ] loss:0.211 acc:40.625 \n","Train | Loss:0.17000 Acc: 95.344\n","Valid | Loss:0.30555 Acc: 79.167 \n","-----------------------------------------------\n","[ Epoch11: 196/196 ] loss:0.245 acc:39.583 \n","Train | Loss:0.16970 Acc: 95.323\n","Valid | Loss:0.30606 Acc: 79.340 \n","saving model with acc 79.340\n","-----------------------------------------------\n","[ Epoch12: 196/196 ] loss:0.123 acc:41.667 \n","Train | Loss:0.16784 Acc: 95.387\n","Valid | Loss:0.30303 Acc: 79.514 \n","saving model with acc 79.514\n","-----------------------------------------------\n","[ Epoch13: 196/196 ] loss:0.135 acc:41.667 \n","Train | Loss:0.16767 Acc: 95.435\n","Valid | Loss:0.30473 Acc: 79.514 \n","-----------------------------------------------\n","[ Epoch14: 196/196 ] loss:0.057 acc:42.708 \n","Train | Loss:0.16723 Acc: 95.382\n","Valid | Loss:0.30100 Acc: 79.514 \n","-----------------------------------------------\n","[ Epoch15: 196/196 ] loss:0.426 acc:38.542 \n","Train | Loss:0.16818 Acc: 95.456\n","Valid | Loss:0.30457 Acc: 78.993 \n","-----------------------------------------------\n","[ Epoch16: 196/196 ] loss:0.303 acc:39.583 \n","Train | Loss:0.16940 Acc: 95.424\n","Valid | Loss:0.30088 Acc: 79.340 \n","-----------------------------------------------\n","[ Epoch17: 196/196 ] loss:0.112 acc:41.667 \n","Train | Loss:0.16640 Acc: 95.499\n","Valid | Loss:0.30260 Acc: 79.514 \n","-----------------------------------------------\n","[ Epoch18: 196/196 ] loss:0.054 acc:42.708 \n","Train | Loss:0.16511 Acc: 95.552\n","Valid | Loss:0.29546 Acc: 79.514 \n","-----------------------------------------------\n","[ Epoch19: 196/196 ] loss:0.109 acc:41.667 \n","Train | Loss:0.16576 Acc: 95.509\n","Valid | Loss:0.29987 Acc: 79.514 \n","-----------------------------------------------\n","[ Epoch20: 196/196 ] loss:0.065 acc:41.667 \n","Train | Loss:0.16515 Acc: 95.599\n","Valid | Loss:0.30028 Acc: 79.514 \n","-----------------------------------------------\n","[ Epoch21: 196/196 ] loss:0.325 acc:38.542 \n","Train | Loss:0.16500 Acc: 95.541\n","Valid | Loss:0.30488 Acc: 79.514 \n","-----------------------------------------------\n","[ Epoch22: 196/196 ] loss:0.124 acc:41.667 \n","Train | Loss:0.16461 Acc: 95.589\n","Valid | Loss:0.29727 Acc: 79.514 \n","-----------------------------------------------\n","[ Epoch23: 196/196 ] loss:0.100 acc:40.625 \n","Train | Loss:0.16401 Acc: 95.562\n","Valid | Loss:0.29540 Acc: 79.688 \n","saving model with acc 79.688\n","-----------------------------------------------\n","[ Epoch24: 196/196 ] loss:0.735 acc:33.333 \n","Train | Loss:0.16536 Acc: 95.594\n","Valid | Loss:0.28973 Acc: 79.688 \n","-----------------------------------------------\n","[ Epoch25: 196/196 ] loss:0.222 acc:39.583 \n","Train | Loss:0.16367 Acc: 95.647\n","Valid | Loss:0.29513 Acc: 79.514 \n","-----------------------------------------------\n","[ Epoch26: 196/196 ] loss:0.300 acc:39.583 \n","Train | Loss:0.16305 Acc: 95.669\n","Valid | Loss:0.29719 Acc: 79.861 \n","saving model with acc 79.861\n","-----------------------------------------------\n","[ Epoch27: 196/196 ] loss:0.195 acc:40.625 \n","Train | Loss:0.16375 Acc: 95.685\n","Valid | Loss:0.29408 Acc: 79.861 \n","-----------------------------------------------\n","[ Epoch28: 196/196 ] loss:0.204 acc:40.625 \n","Train | Loss:0.16153 Acc: 95.685\n","Valid | Loss:0.29306 Acc: 79.688 \n","-----------------------------------------------\n","[ Epoch29: 196/196 ] loss:0.185 acc:40.625 \n","Train | Loss:0.16156 Acc: 95.748\n","Valid | Loss:0.27694 Acc: 79.688 \n","-----------------------------------------------\n","[ Epoch30: 196/196 ] loss:0.263 acc:39.583 \n","Train | Loss:0.16231 Acc: 95.690\n","Valid | Loss:0.27329 Acc: 79.514 \n","-----------------------------------------------\n","[ Epoch31: 196/196 ] loss:0.225 acc:40.625 \n","Train | Loss:0.16031 Acc: 95.732\n","Valid | Loss:0.28429 Acc: 79.514 \n","-----------------------------------------------\n","[ Epoch32: 196/196 ] loss:0.195 acc:40.625 \n","Train | Loss:0.16215 Acc: 95.685\n","Valid | Loss:0.28190 Acc: 79.514 \n","-----------------------------------------------\n","[ Epoch33: 196/196 ] loss:0.295 acc:39.583 \n","Train | Loss:0.15915 Acc: 95.647\n","Valid | Loss:0.26512 Acc: 79.514 \n","-----------------------------------------------\n","[ Epoch34: 196/196 ] loss:0.139 acc:40.625 \n","Train | Loss:0.15749 Acc: 95.685\n","Valid | Loss:0.29951 Acc: 79.340 \n","-----------------------------------------------\n","[ Epoch35: 196/196 ] loss:0.367 acc:38.542 \n","Train | Loss:0.15532 Acc: 95.732\n","Valid | Loss:0.27890 Acc: 79.514 \n","-----------------------------------------------\n","[ Epoch36: 196/196 ] loss:0.243 acc:40.625 \n","Train | Loss:0.15356 Acc: 95.621\n","Valid | Loss:0.26153 Acc: 79.688 \n","-----------------------------------------------\n","[ Epoch37: 196/196 ] loss:0.033 acc:42.708 \n","Train | Loss:0.15135 Acc: 95.695\n","Valid | Loss:0.27019 Acc: 79.688 \n","-----------------------------------------------\n","[ Epoch38: 196/196 ] loss:0.244 acc:40.625 \n","Train | Loss:0.15005 Acc: 95.711\n","Valid | Loss:0.26048 Acc: 79.514 \n","-----------------------------------------------\n","[ Epoch39: 196/196 ] loss:0.110 acc:40.625 \n","Train | Loss:0.14777 Acc: 95.685\n","Valid | Loss:0.25523 Acc: 79.688 \n","-----------------------------------------------\n","[ Epoch40: 196/196 ] loss:0.053 acc:41.667 \n","Train | Loss:0.14814 Acc: 95.711\n","Valid | Loss:0.25759 Acc: 79.514 \n","-----------------------------------------------\n","[ Epoch41: 196/196 ] loss:0.171 acc:39.583 \n","Train | Loss:0.14686 Acc: 95.754\n","Valid | Loss:0.27553 Acc: 79.688 \n","-----------------------------------------------\n","[ Epoch42: 196/196 ] loss:0.090 acc:41.667 \n","Train | Loss:0.14484 Acc: 95.775\n","Valid | Loss:0.25807 Acc: 79.688 \n","-----------------------------------------------\n","[ Epoch43: 196/196 ] loss:0.265 acc:40.625 \n","Train | Loss:0.14717 Acc: 95.685\n","Valid | Loss:0.24522 Acc: 79.688 \n","-----------------------------------------------\n","[ Epoch44: 196/196 ] loss:0.092 acc:41.667 \n","Train | Loss:0.14857 Acc: 95.493\n","Valid | Loss:0.25938 Acc: 79.688 \n","-----------------------------------------------\n","[ Epoch45: 196/196 ] loss:0.156 acc:41.667 \n","Train | Loss:0.14419 Acc: 95.770\n","Valid | Loss:0.25326 Acc: 79.688 \n","-----------------------------------------------\n","[ Epoch46: 196/196 ] loss:0.143 acc:40.625 \n","Train | Loss:0.14161 Acc: 95.796\n","Valid | Loss:0.24284 Acc: 79.688 \n","-----------------------------------------------\n","[ Epoch47: 196/196 ] loss:0.219 acc:38.542 \n","Train | Loss:0.14296 Acc: 95.754\n","Valid | Loss:0.29299 Acc: 76.910 \n","-----------------------------------------------\n","[ Epoch48: 196/196 ] loss:0.215 acc:39.583 \n","Train | Loss:0.15574 Acc: 95.137\n","Valid | Loss:0.24828 Acc: 79.688 \n","-----------------------------------------------\n","[ Epoch49: 196/196 ] loss:0.040 acc:41.667 \n","Train | Loss:0.14051 Acc: 95.770\n","Valid | Loss:0.25913 Acc: 79.688 \n","-----------------------------------------------\n","[ Epoch50: 196/196 ] loss:0.064 acc:42.708 \n","Train | Loss:0.14168 Acc: 95.727\n","Valid | Loss:0.24732 Acc: 79.688 \n","-----------------------------------------------\n"]}]},{"cell_type":"markdown","source":["# Predict and save to csv file"],"metadata":{"id":"g4eX2bMs9LNC"}},{"cell_type":"code","source":["print(\"loading testing data ...\")\n","test_x = load_testing_data(testing_data)\n","preprocess = Preprocess(test_x, sen_len, w2v_path=w2v_path)\n","embedding = preprocess.make_embedding(load=True)\n","test_x = preprocess.sentence_word2idx()\n","test_dataset = TwitterDataset(X=test_x, y=None)\n","test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n","                            batch_size = batch_size,\n","                            shuffle = False,\n","                            num_workers = 8)\n","\n","#test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n","#                           batch_size = batch_size,\n","#                           shuffle = False,\n","#                           num_workers = 0)\n","print('\\nload model ...')\n","model = torch.load(os.path.join(model_dir, '/content/ckpt.model'))\n","outputs = testing(batch_size, test_loader, model, device)\n","\n","# save as csv\n","tmp = pd.DataFrame({\"id\":[str(i) for i in range(len(test_x))],\"labels\":outputs})\n","print(\"save csv ...\")\n","tmp.to_csv(os.path.join(path_prefix, 'Dissertation_predict.csv'), index=False)\n","tmp.to_csv('/content/drive/My Drive/Dissertation/Dissertation_predict.csv', index=False)\n","\n","#from google.colab import drive\n","#drive.flush_and_unmount()\n","\n","print(\"Finish Predicting\")"],"metadata":{"id":"qmbBBo0a9IJk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689495372290,"user_tz":-480,"elapsed":21232,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}},"outputId":"dead12e5-b8ad-424a-a3d7-34310a8c7ea4"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["loading testing data ...\n","Get embedding ...\n","loading word to vec model ...\n","get words #73201\n","total words: 73203\n","sentence count #671\n","load model ...\n","save csv ...\n","Finish Predicting\n"]}]},{"cell_type":"code","source":["print(tmp.loc[tmp['labels'] == 1])\n","print(len(tmp.loc[tmp['labels'] == 1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RXvg8Znd94Z9","executionInfo":{"status":"ok","timestamp":1689494971345,"user_tz":-480,"elapsed":483,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}},"outputId":"a4295401-9e64-4c4a-d8eb-b33c9dfe3861"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["      id  labels\n","34    34       1\n","55    55       1\n","77    77       1\n","82    82       1\n","104  104       1\n","198  198       1\n","207  207       1\n","289  289       1\n","291  291       1\n","294  294       1\n","301  301       1\n","318  318       1\n","344  344       1\n","436  436       1\n","519  519       1\n","522  522       1\n","566  566       1\n","580  580       1\n","604  604       1\n","657  657       1\n","20\n"]}]},{"cell_type":"code","source":["\n","test_verification = pd.read_excel('/content/drive/MyDrive/Dissertation/test_verification.xlsx')\n","y_true = test_verification['labels']\n","\n","from sklearn.metrics import f1_score\n","\n","print(f1_score(y_true, tmp['labels'], average='binary'))\n","\n","print(len(y_true))\n","print(len(tmp['labels']))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EsYLVixRD_IJ","executionInfo":{"status":"ok","timestamp":1689495372291,"user_tz":-480,"elapsed":14,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}},"outputId":"22a333a2-9312-4aa5-fc67-5e87236bd87d"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["0.2784810126582279\n","671\n","671\n"]}]},{"cell_type":"code","source":["#drive.flush_and_unmount()\n","\n","from sklearn.metrics import classification_report\n","\n","print(classification_report(y_true, tmp['labels']))"],"metadata":{"id":"bg_TfDArKZMR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689495376071,"user_tz":-480,"elapsed":4,"user":{"displayName":"Lavi Lucky","userId":"01327156043720177521"}},"outputId":"000b2468-79d7-41da-ba04-b2164018df33"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.92      0.99      0.95       610\n","           1       0.61      0.18      0.28        61\n","\n","    accuracy                           0.92       671\n","   macro avg       0.77      0.58      0.62       671\n","weighted avg       0.90      0.92      0.89       671\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"kYmuGbbfv9fC"},"execution_count":null,"outputs":[]}]}