{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1p03sk2EPv1QAhfC7513PLtg7W8qyYH3V",
      "authorship_tag": "ABX9TyPj/KnftTODoq+4+BN2JYXp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LaviWhite/Story-Annotation/blob/main/GRU_Product_Comparsion_Review/GRU_Product_Comparsion_Review_supervised_sample_weight_75.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Sampling"
      ],
      "metadata": {
        "id": "sahNfYZRCCXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "pnoaocekiH0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/drive/MyDrive/Dissertation/Original.xlsx')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "2MPuhKg8jBum",
        "outputId": "1aeca782-8da8-4fe2-eaf3-91b7086de807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   review_cnt is_hot        publish_time                        id  \\\n",
              "0        84.0    NaN 2021-02-10 12:10:00  60235cc900000000010282bf   \n",
              "1        76.0    NaN 2021-02-06 12:05:00  601e158f000000000101f856   \n",
              "2         0.0     爆文 2021-02-16 14:33:00  602b6729000000002103e7ff   \n",
              "3      1182.0     爆文 2021-02-10 17:00:00  6023a0c7000000002103ecd0   \n",
              "4       523.0     爆文 2021-02-03 22:32:00  601ab3fd000000000101f4b2   \n",
              "\n",
              "  is_original                                           pic_urls profilers_id  \\\n",
              "0        普通笔记  [http://sns-img-qc.xhscdn.com/e12a7183-0d2b-3c...     [741974]   \n",
              "1        普通笔记  [http://sns-img-hw.xhscdn.com/3795c2fe-1819-34...     [739701]   \n",
              "2        视频笔记  [http://sns-img-qc.xhscdn.com/6fd88377-9ffa-34...     [747652]   \n",
              "3        视频笔记  [\"http://sns-img-qc.xhscdn.com/9f6a4597-1fd3-3...   [\"859906\"]   \n",
              "4        视频笔记  [\"http://sns-img-qc.xhscdn.com/38e07503-6677-3...   [\"859912\"]   \n",
              "\n",
              "                                          author_url  site_id  fans_cnt  ...  \\\n",
              "0  http://www.xiaohongshu.com/user/profile/5a2ff3...   146510    464770  ...   \n",
              "1  http://www.xiaohongshu.com/user/profile/5a2ff3...   146510    457941  ...   \n",
              "2  http://www.xiaohongshu.com/user/profile/5e8f86...   146510    555822  ...   \n",
              "3  http://www.xiaohongshu.com/user/profile/565f83...   146510    535219  ...   \n",
              "4  http://www.xiaohongshu.com/user/profile/55d6d2...   146510    856274  ...   \n",
              "\n",
              "      red_id video_cnt                   item_id follow_cnt coin_cnt  \\\n",
              "0  420599937        94  60235cc900000000010282bf        185  2386973   \n",
              "1  420599937        92  601e158f000000000101f856        185  2341527   \n",
              "2    T654837       125  602b6729000000002103e7ff          7  2106984   \n",
              "3  494694428       438  6023a0c7000000002103ecd0        111  2150628   \n",
              "4   Qbaby414       416  601ab3fd000000000101f4b2          1  3099241   \n",
              "\n",
              "  data_type  user_location  entity_keywords.keyword  entity_keywords.type  \\\n",
              "0      电商笔记         浙江  杭州                      NaN                   NaN   \n",
              "1      电商笔记         浙江  杭州                      NaN                   NaN   \n",
              "2      电商笔记         广西  南宁                      NaN                   NaN   \n",
              "3      电商笔记             中国                      NaN                   NaN   \n",
              "4      电商笔记         陕西  西安                      NaN                   NaN   \n",
              "\n",
              "  cooperation_brand_names  \n",
              "0                     NaN  \n",
              "1                     NaN  \n",
              "2                     NaN  \n",
              "3                     NaN  \n",
              "4                     NaN  \n",
              "\n",
              "[5 rows x 49 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9f6708d2-6457-4822-9f9b-fdbf1bf65c15\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_cnt</th>\n",
              "      <th>is_hot</th>\n",
              "      <th>publish_time</th>\n",
              "      <th>id</th>\n",
              "      <th>is_original</th>\n",
              "      <th>pic_urls</th>\n",
              "      <th>profilers_id</th>\n",
              "      <th>author_url</th>\n",
              "      <th>site_id</th>\n",
              "      <th>fans_cnt</th>\n",
              "      <th>...</th>\n",
              "      <th>red_id</th>\n",
              "      <th>video_cnt</th>\n",
              "      <th>item_id</th>\n",
              "      <th>follow_cnt</th>\n",
              "      <th>coin_cnt</th>\n",
              "      <th>data_type</th>\n",
              "      <th>user_location</th>\n",
              "      <th>entity_keywords.keyword</th>\n",
              "      <th>entity_keywords.type</th>\n",
              "      <th>cooperation_brand_names</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>84.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2021-02-10 12:10:00</td>\n",
              "      <td>60235cc900000000010282bf</td>\n",
              "      <td>普通笔记</td>\n",
              "      <td>[http://sns-img-qc.xhscdn.com/e12a7183-0d2b-3c...</td>\n",
              "      <td>[741974]</td>\n",
              "      <td>http://www.xiaohongshu.com/user/profile/5a2ff3...</td>\n",
              "      <td>146510</td>\n",
              "      <td>464770</td>\n",
              "      <td>...</td>\n",
              "      <td>420599937</td>\n",
              "      <td>94</td>\n",
              "      <td>60235cc900000000010282bf</td>\n",
              "      <td>185</td>\n",
              "      <td>2386973</td>\n",
              "      <td>电商笔记</td>\n",
              "      <td>浙江  杭州</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>76.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2021-02-06 12:05:00</td>\n",
              "      <td>601e158f000000000101f856</td>\n",
              "      <td>普通笔记</td>\n",
              "      <td>[http://sns-img-hw.xhscdn.com/3795c2fe-1819-34...</td>\n",
              "      <td>[739701]</td>\n",
              "      <td>http://www.xiaohongshu.com/user/profile/5a2ff3...</td>\n",
              "      <td>146510</td>\n",
              "      <td>457941</td>\n",
              "      <td>...</td>\n",
              "      <td>420599937</td>\n",
              "      <td>92</td>\n",
              "      <td>601e158f000000000101f856</td>\n",
              "      <td>185</td>\n",
              "      <td>2341527</td>\n",
              "      <td>电商笔记</td>\n",
              "      <td>浙江  杭州</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>爆文</td>\n",
              "      <td>2021-02-16 14:33:00</td>\n",
              "      <td>602b6729000000002103e7ff</td>\n",
              "      <td>视频笔记</td>\n",
              "      <td>[http://sns-img-qc.xhscdn.com/6fd88377-9ffa-34...</td>\n",
              "      <td>[747652]</td>\n",
              "      <td>http://www.xiaohongshu.com/user/profile/5e8f86...</td>\n",
              "      <td>146510</td>\n",
              "      <td>555822</td>\n",
              "      <td>...</td>\n",
              "      <td>T654837</td>\n",
              "      <td>125</td>\n",
              "      <td>602b6729000000002103e7ff</td>\n",
              "      <td>7</td>\n",
              "      <td>2106984</td>\n",
              "      <td>电商笔记</td>\n",
              "      <td>广西  南宁</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1182.0</td>\n",
              "      <td>爆文</td>\n",
              "      <td>2021-02-10 17:00:00</td>\n",
              "      <td>6023a0c7000000002103ecd0</td>\n",
              "      <td>视频笔记</td>\n",
              "      <td>[\"http://sns-img-qc.xhscdn.com/9f6a4597-1fd3-3...</td>\n",
              "      <td>[\"859906\"]</td>\n",
              "      <td>http://www.xiaohongshu.com/user/profile/565f83...</td>\n",
              "      <td>146510</td>\n",
              "      <td>535219</td>\n",
              "      <td>...</td>\n",
              "      <td>494694428</td>\n",
              "      <td>438</td>\n",
              "      <td>6023a0c7000000002103ecd0</td>\n",
              "      <td>111</td>\n",
              "      <td>2150628</td>\n",
              "      <td>电商笔记</td>\n",
              "      <td>中国</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>523.0</td>\n",
              "      <td>爆文</td>\n",
              "      <td>2021-02-03 22:32:00</td>\n",
              "      <td>601ab3fd000000000101f4b2</td>\n",
              "      <td>视频笔记</td>\n",
              "      <td>[\"http://sns-img-qc.xhscdn.com/38e07503-6677-3...</td>\n",
              "      <td>[\"859912\"]</td>\n",
              "      <td>http://www.xiaohongshu.com/user/profile/55d6d2...</td>\n",
              "      <td>146510</td>\n",
              "      <td>856274</td>\n",
              "      <td>...</td>\n",
              "      <td>Qbaby414</td>\n",
              "      <td>416</td>\n",
              "      <td>601ab3fd000000000101f4b2</td>\n",
              "      <td>1</td>\n",
              "      <td>3099241</td>\n",
              "      <td>电商笔记</td>\n",
              "      <td>陕西  西安</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 49 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f6708d2-6457-4822-9f9b-fdbf1bf65c15')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9f6708d2-6457-4822-9f9b-fdbf1bf65c15 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9f6708d2-6457-4822-9f9b-fdbf1bf65c15');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = df['content'].sample(n=10000, random_state=1)\n",
        "sample.to_excel(\"/content/randomsample.xlsx\")"
      ],
      "metadata": {
        "id": "J5D_fG6mlIcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Drive for saving"
      ],
      "metadata": {
        "id": "BfcDHbR5iE77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lu9Cd8DaiCSv",
        "outputId": "a56734c2-1776-486e-aebc-c185daf70d58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "O4jNp3mDPYZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jieba"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQBDRkNkxmsy",
        "outputId": "7d78d7f5-8679-4fbb-d8e6-7d8e650a86a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (0.42.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python -m spacy download zh_core_web_trf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UXOmiLYF2eU_",
        "outputId": "3444f611-078f-4810-b2c1-153d5ac7841a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-23.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (67.7.2)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-68.0.0-py3-none-any.whl (804 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m804.0/804.0 kB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.40.0)\n",
            "Installing collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-23.2 setuptools-68.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "setuptools"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.4)\n",
            "Collecting spacy\n",
            "  Obtaining dependency information for spacy from https://files.pythonhosted.org/packages/8f/bc/213638f0242858d936c171f4e32e431a6fbbe703184a944a81c065cdc414/spacy-3.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading spacy-3.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.11)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (68.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "Downloading spacy-3.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: spacy\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.5.4\n",
            "    Uninstalling spacy-3.5.4:\n",
            "      Successfully uninstalled spacy-3.5.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 3.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed spacy-3.6.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m2023-07-22 07:34:40.299322: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-22 07:34:40.358690: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-22 07:34:41.385473: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting zh-core-web-trf==3.6.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/zh_core_web_trf-3.6.1/zh_core_web_trf-3.6.1-py3-none-any.whl (417.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.4/417.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from zh-core-web-trf==3.6.1) (3.6.0)\n",
            "Collecting spacy-transformers<1.3.0,>=1.2.2 (from zh-core-web-trf==3.6.1)\n",
            "  Obtaining dependency information for spacy-transformers<1.3.0,>=1.2.2 from https://files.pythonhosted.org/packages/16/40/892e39e29ed984b29507d69c4076429a3e9404b56e79c00e2a8d9b9cc1e8/spacy_transformers-1.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading spacy_transformers-1.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting spacy-pkuseg<0.1.0,>=0.0.27 (from zh-core-web-trf==3.6.1)\n",
            "  Downloading spacy_pkuseg-0.0.32-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (2.0.8)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (1.10.11)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (68.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (3.3.0)\n",
            "Collecting transformers<4.31.0,>=3.4.0 (from spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1)\n",
            "  Obtaining dependency information for transformers<4.31.0,>=3.4.0 from https://files.pythonhosted.org/packages/5b/0b/e45d26ccd28568013523e04f325432ea88a442b4e3020b757cf4361f0120/transformers-4.30.2-py3-none-any.whl.metadata\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl.metadata (113 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (2.0.1+cu118)\n",
            "Collecting spacy-alignments<1.0.0,>=0.7.2 (from spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1)\n",
            "  Downloading spacy_alignments-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (0.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (16.0.6)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1)\n",
            "  Obtaining dependency information for huggingface-hub<1.0,>=0.14.1 from https://files.pythonhosted.org/packages/7f/c4/adcbe9a696c135578cabcbdd7331332daad4d49b7c43688bc2d36b3a47d2/huggingface_hub-0.16.4-py3-none-any.whl.metadata\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->zh-core-web-trf==3.6.1) (2.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (2023.6.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->zh-core-web-trf==3.6.1) (1.3.0)\n",
            "Downloading spacy_transformers-1.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (190 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.8/190.8 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, safetensors, spacy-alignments, spacy-pkuseg, huggingface-hub, transformers, spacy-transformers, zh-core-web-trf\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 spacy-alignments-0.9.0 spacy-pkuseg-0.0.32 spacy-transformers-1.2.5 tokenizers-0.13.3 transformers-4.30.2 zh-core-web-trf-3.6.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('zh_core_web_trf')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import jieba\n",
        "import jieba.analyse\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "g3jxI-N6U3aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopwordslist():\n",
        "        stopwords = [line.strip() for line in open('/content/drive/MyDrive/Dissertation/stopwords.txt',encoding='utf-8').readlines()]\n",
        "        return stopwords\n",
        "\n",
        "def seg_depart(sentence):\n",
        "    sentence_depart = jieba.lcut(sentence)\n",
        "    stopwords = stopwordslist()\n",
        "    outstr=''\n",
        "    for word in sentence_depart:\n",
        "        if word not in stopwords:\n",
        "            outstr += word\n",
        "            outstr += ' '\n",
        "    return outstr"
      ],
      "metadata": {
        "id": "IAZOZ9uVDjqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Segmentation of Train set (without label)"
      ],
      "metadata": {
        "id": "CJCKrgpwevm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into testing set and training without label set and output as txt file\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "dt = pd.read_excel('/content/drive/MyDrive/Dissertation/Original.xlsx')\n",
        "X = dt['content']\n",
        "\n",
        "train, test = train_test_split(X, test_size=0.005, random_state=2)\n",
        "\n",
        "#test.to_excel(\"/content/drive/MyDrive/Dissertation/test_verification.xlsx\")"
      ],
      "metadata": {
        "id": "1LQCIT4QfR0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_train = []\n",
        "\n",
        "for line in train.astype(str):\n",
        "  line_seg=seg_depart(line)\n",
        "  output_train.append(line_seg)\n",
        "print('success！') # output_train is the segmented list for training(without label) set"
      ],
      "metadata": {
        "id": "aZGn9fkbee-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "712b8748-b217-44a2-a4a9-cba41f3cd1df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.694 seconds.\n",
            "DEBUG:jieba:Loading model cost 0.694 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "success！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('/content/drive/MyDrive/Dissertation/Train_nolabel.txt','w')\n",
        "for item in output_train:\n",
        "\tfile.write(item+\"\\n\")\n",
        "file.close()\n",
        "print('success')"
      ],
      "metadata": {
        "id": "fyzo6rz12bz1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df83d70f-8b09-43b8-c8e2-fd893bc55c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "success\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Segmentation of Testing set (without label)"
      ],
      "metadata": {
        "id": "jnkcyCzd3Jf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_test = []\n",
        "\n",
        "for line in test.astype(str):\n",
        "  line_seg=seg_depart(line)\n",
        "  output_test.append(line_seg)\n",
        "print('success！') # output_test is the segmented list for testing set"
      ],
      "metadata": {
        "id": "ud5Ikvqd3BoK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "1eee2ced-913a-497a-d36b-535e83576c93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-d8758046b4c1>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutput_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mline_seg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseg_depart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0moutput_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_seg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('/content/drive/MyDrive/Dissertation/Test.txt','w')\n",
        "for item in output_test:\n",
        "\tfile.write(item+\"\\n\")\n",
        "file.close()\n",
        "print('success！')"
      ],
      "metadata": {
        "id": "Zsl5lKUl3C0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Segmentation of Training Set (with label)"
      ],
      "metadata": {
        "id": "EeFLgiWkB7tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dr = pd.read_excel('/content/drive/MyDrive/Dissertation/randomsample.xlsx', index_col='index')\n",
        "#content = df['content'].tolist()\n",
        "\n",
        "dr1 = dr.loc[dr['label'] == 1]\n",
        "dr0 = dr.loc[dr['label'] == 0]\n",
        "\n",
        "print('Sample size before downsize: '+str(len(dr)))\n",
        "print('Positive sample size before downsize: '+str(len(dr1)))\n",
        "print('Negative sample size before downsize: '+str(len(dr0)))\n",
        "\n",
        "dr0 = dr0.sample(frac=0.032, replace=True, random_state=1) #downsize the major group\n",
        "# 0.015 resulted 15 positive sample\n",
        "\n",
        "\n",
        "df = pd.concat([dr1,dr0], ignore_index=True, sort=False)\n",
        "\n",
        "df = df.sample(frac=1).reset_index() #shuffle the order of the trainning sample\n",
        "\n",
        "label = df['label'].tolist()\n",
        "\n",
        "print('Sample size after downsize: '+str(len(df)))\n",
        "print('Positive sample size after downsize: '+str(len(df.loc[df['label']== 1])))\n",
        "print('Negative sample size after downsize: '+str(len(df.loc[df['label']== 0])))\n",
        "print('Proportion for Positive sample after downsize: '+str(len(df.loc[df['label']== 1])/len(df)))"
      ],
      "metadata": {
        "id": "-6jEjP3SuCgy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6b742a0-1dba-4afe-ed31-c9cf7d44f899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample size before downsize: 10000\n",
            "Positive sample size before downsize: 879\n",
            "Negative sample size before downsize: 9121\n",
            "Sample size after downsize: 1171\n",
            "Positive sample size after downsize: 879\n",
            "Negative sample size after downsize: 292\n",
            "Proportion for Positive sample after downsize: 0.7506404782237404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = []\n",
        "\n",
        "for line in df['content'].astype(str):\n",
        "  line_seg=seg_depart(line)\n",
        "  output.append(line_seg) # output is the segmented list for training(with label) set\n",
        "print('success！')"
      ],
      "metadata": {
        "id": "SKeXmaX2cHcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88abeac5-a943-4480-f4b8-9d1e0c18361b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.746 seconds.\n",
            "DEBUG:jieba:Loading model cost 0.746 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "success！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ele = [\" +++$+++ \"]\n",
        "element = ele * len(df) # add separator element to separate label and content\n",
        "#print(element)\n",
        "\n",
        "concat_function = lambda x,y: str(x) + \"\" + y # change label as string and concat separator element\n",
        "concat_func = lambda x,y: x + \"\" + y # concat label and training set\n",
        "\n",
        "a = list(map(concat_function,label,element))\n",
        "\n",
        "labels = list(map(concat_func,a,output))"
      ],
      "metadata": {
        "id": "2I9nu3JQ03kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output as txt file Train with label\n",
        "\n",
        "file = open('/content/drive/MyDrive/Dissertation/Train_label.txt','w')\n",
        "for item in labels:\n",
        "\tfile.write(item+\"\\n\")\n",
        "file.close()\n",
        "\n",
        "print('success！')"
      ],
      "metadata": {
        "id": "-ShBP7Rkef3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dec87ee-cf77-489c-c0f5-1cfdf026f555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "success！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "z-1Abhci7qxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "path_prefix = './'"
      ],
      "metadata": {
        "id": "4JUeQbbf7yNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def load_training_data(path='/content/drive/MyDrive/Dissertation/Train_label.txt'):\n",
        "    # Read training data\n",
        "    if 'Train_label' in path:\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "            lines = [line.strip('\\n').split(' ') for line in lines]\n",
        "        x = [line[2:] for line in lines]\n",
        "        y = [line[0] for line in lines]\n",
        "        return x, y\n",
        "    else:\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "            x = [line.strip('\\n').split(' ') for line in lines]\n",
        "        return x\n",
        "\n",
        "def load_testing_data(path='/content/drive/MyDrive/Dissertation/Test.txt'):\n",
        "    # Read testing data\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "        X = [\"\".join(line.strip('\\n').split(\",\")).strip() for line in lines]\n",
        "        X = [sen.split(' ') for sen in X]\n",
        "    return X\n",
        "\n",
        "def evaluation(outputs, labels):\n",
        "    #outputs => probability (float)\n",
        "    #labels => labels\n",
        "    outputs[outputs>=0.5] = 1 # Negtive Sentiment\n",
        "    outputs[outputs<0.5] = 0 # Positive Sentiment\n",
        "    correct = torch.sum(torch.eq(outputs, labels)).item()\n",
        "    return correct"
      ],
      "metadata": {
        "id": "LUzQlX5x799e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Word to Vector"
      ],
      "metadata": {
        "id": "3Xj-En_J8E6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import argparse\n",
        "from gensim.models import word2vec\n",
        "\n",
        "def train_word2vec(x):\n",
        "    model = word2vec.Word2Vec(x, vector_size=250, window=5, min_count=5, workers=12)\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"loading training data ...\")\n",
        "    train_x, y = load_training_data('/content/drive/MyDrive/Dissertation/Train_label.txt')\n",
        "    #train_x_no_label = load_training_data('/content/drive/MyDrive/Dissertation/Train_nolabel.txt')\n",
        "\n",
        "    print(\"loading testing data ...\")\n",
        "    test_x = load_testing_data('/content/drive/MyDrive/Dissertation/Test.txt')\n",
        "\n",
        "    #model = train_word2vec(train_x + train_x_no_label + test_x)\n",
        "    model = train_word2vec(train_x + test_x)\n",
        "\n",
        "    print(\"saving model ...\")\n",
        "    model.save(os.path.join(path_prefix, '/w2v_all.model'))"
      ],
      "metadata": {
        "id": "bnfXyCOh8A0G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "336aa22b-4c35-4d92-b27e-ac25462ec00f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading training data ...\n",
            "loading testing data ...\n",
            "saving model ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocess"
      ],
      "metadata": {
        "id": "KfFTVTll8Xt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "class Preprocess():\n",
        "    def __init__(self, sentences, sen_len, w2v_path=\"./w2v.model\"):\n",
        "        self.w2v_path = w2v_path\n",
        "        self.sentences = sentences\n",
        "        self.sen_len = sen_len\n",
        "        self.idx2word = []\n",
        "        self.word2idx = {}\n",
        "        self.embedding_matrix = []\n",
        "    def get_w2v_model(self):\n",
        "        # load word to vector model\n",
        "        self.embedding = Word2Vec.load(self.w2v_path)\n",
        "        self.embedding_dim = self.embedding.vector_size\n",
        "    def add_embedding(self, word):\n",
        "        # add word into embedding\n",
        "        vector = torch.empty(1, self.embedding_dim)\n",
        "        torch.nn.init.uniform_(vector)\n",
        "        self.word2idx[word] = len(self.word2idx)\n",
        "        self.idx2word.append(word)\n",
        "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)\n",
        "    def make_embedding(self, load=True):\n",
        "        print(\"Get embedding ...\")\n",
        "        if load:\n",
        "            print(\"loading word to vec model ...\")\n",
        "            self.get_w2v_model()\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        for i, word in enumerate(self.embedding.wv.key_to_index):\n",
        "            print('get words #{}'.format(i+1), end='\\r')\n",
        "            self.word2idx[word] = len(self.word2idx)\n",
        "            self.idx2word.append(word)\n",
        "            self.embedding_matrix.append(self.embedding.wv[word])\n",
        "        print('')\n",
        "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
        "        self.add_embedding(\"<PAD>\")\n",
        "        self.add_embedding(\"<UNK>\")\n",
        "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
        "        return self.embedding_matrix\n",
        "    def pad_sequence(self, sentence):\n",
        "        if len(sentence) > self.sen_len:\n",
        "            sentence = sentence[:self.sen_len]\n",
        "        else:\n",
        "            pad_len = self.sen_len - len(sentence)\n",
        "            for _ in range(pad_len):\n",
        "                sentence.append(self.word2idx[\"<PAD>\"])\n",
        "        assert len(sentence) == self.sen_len\n",
        "        return sentence\n",
        "    def sentence_word2idx(self):\n",
        "        sentence_list = []\n",
        "        for i, sen in enumerate(self.sentences):\n",
        "            print('sentence count #{}'.format(i+1), end='\\r')\n",
        "            sentence_idx = []\n",
        "            for word in sen:\n",
        "                if (word in self.word2idx.keys()):\n",
        "                    sentence_idx.append(self.word2idx[word])\n",
        "                else:\n",
        "                    sentence_idx.append(self.word2idx[\"<UNK>\"])\n",
        "            sentence_idx = self.pad_sequence(sentence_idx)\n",
        "            sentence_list.append(sentence_idx)\n",
        "        return torch.LongTensor(sentence_list)\n",
        "    def labels_to_tensor(self, y):\n",
        "        # turn labels into tensors\n",
        "        y = [int(label) for label in y]\n",
        "        return torch.LongTensor(y)"
      ],
      "metadata": {
        "id": "xECUJfRy8VhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "YN0qajNL8ejJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils import data\n",
        "\n",
        "class TwitterDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    Expected data shape like:(data_num, data_len)\n",
        "    Data can be a list of numpy array or a list of lists\n",
        "    input data shape : (data_num, seq_len, feature_dim)\n",
        "\n",
        "    __len__ will return the number of data\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y):\n",
        "        self.data = X\n",
        "        self.label = y\n",
        "    def __getitem__(self, idx):\n",
        "        if self.label is None: return self.data[idx]\n",
        "        return self.data[idx], self.label[idx]\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "pidDfJOu8dii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Model"
      ],
      "metadata": {
        "id": "Y5QzeI6i8kFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "class GRU_Net(nn.Module):\n",
        "    def __init__(self, embedding, embedding_dim, hidden_dim, num_layers, dropout=0.5, fix_embedding=True):\n",
        "        super(GRU_Net, self).__init__()\n",
        "        # embedding layer\n",
        "        self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))\n",
        "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
        "        # Whether fix embedding\n",
        "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
        "        self.embedding_dim = embedding.size(1)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.classifier = nn.Sequential( nn.Dropout(dropout),\n",
        "                          nn.Linear(hidden_dim, 1),\n",
        "                          nn.Sigmoid() )\n",
        "    def forward(self, inputs):\n",
        "        inputs = self.embedding(inputs)\n",
        "        x, _ = self.gru(inputs, None)\n",
        "        # dimension of x (batch, seq_len, hidden_size)\n",
        "        x = x[:, -1, :]\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "c-0IPClf8nyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Training"
      ],
      "metadata": {
        "id": "RoKr7ggM8s6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def training(batch_size, n_epoch, lr, model_dir, train, valid, model, device):\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print('\\nstart training, parameter total:{}, trainable:{}\\n'.format(total, trainable))\n",
        "    model.train() # set training mode\n",
        "    criterion = nn.BCELoss() # Define loss function\n",
        "    t_batch = len(train)\n",
        "    v_batch = len(valid)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr) # set optimizer as Adam\n",
        "    total_loss, total_acc, best_acc = 0, 0, 0\n",
        "    epoch_num=range(1,6)#1,2,3,4,5\n",
        "    train_loss=[]\n",
        "    valid_loss=[]\n",
        "    for epoch in range(n_epoch):\n",
        "        total_loss, total_acc = 0, 0\n",
        "\n",
        "        # For training\n",
        "        model.train() #set training mode\n",
        "        for i, (inputs, labels) in enumerate(train):\n",
        "            inputs = inputs.to(device, dtype=torch.long) # set device \"cuda\"\n",
        "            labels = labels.to(device, dtype=torch.float) # set device \"cuda\"\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            outputs = outputs.squeeze()\n",
        "            model.train()\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            correct = evaluation(outputs, labels) # calculate accuracy\n",
        "            total_acc += (correct / batch_size)\n",
        "            total_loss += loss.item()\n",
        "            print('[ Epoch{}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(\n",
        "            \tepoch+1, i+1, t_batch, loss.item(), correct*100/batch_size), end='\\r')\n",
        "        print('\\nTrain | Loss:{:.5f} Acc: {:.3f}'.format(total_loss/t_batch, total_acc/t_batch*100))\n",
        "        train_loss.append(total_loss/t_batch)\n",
        "\n",
        "        # For validation\n",
        "        model.eval() # set validation mode\n",
        "        with torch.no_grad():\n",
        "            total_loss, total_acc = 0, 0\n",
        "            for i, (inputs, labels) in enumerate(valid):\n",
        "                inputs = inputs.to(device, dtype=torch.long) # set device \"cuda\"\n",
        "                labels = labels.to(device, dtype=torch.float) # set device \"cuda\"\n",
        "                outputs = model(inputs)\n",
        "                outputs = outputs.squeeze()\n",
        "                loss = criterion(outputs, labels)\n",
        "                correct = evaluation(outputs, labels)\n",
        "                total_acc += (correct / batch_size)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss/v_batch, total_acc/v_batch*100))\n",
        "            if total_acc > best_acc:\n",
        "                # if the result of validation is better than previous model, save the new model\n",
        "                best_acc = total_acc\n",
        "                torch.save(model, \"{}/ckpt.model\".format(model_dir))\n",
        "                print('saving model with acc {:.3f}'.format(total_acc/v_batch*100))\n",
        "        print('-----------------------------------------------')\n",
        "        valid_loss.append(total_loss/v_batch)"
      ],
      "metadata": {
        "id": "VxpLcHOW8rjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Pesudo Labelling"
      ],
      "metadata": {
        "id": "HaxC3Y0a-NCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "def got_label(batch_size, test_loader, model, device):\n",
        "    model.eval()\n",
        "    ret_output = []\n",
        "    with torch.no_grad():\n",
        "        for i, inputs in enumerate(test_loader):\n",
        "            inputs = inputs.to(device, dtype=torch.long)\n",
        "            outputs = model(inputs)\n",
        "            outputs = outputs.squeeze()\n",
        "            #outputs[outputs>=0.9] = 1 # cannot change into 1 first\n",
        "            #outputs[outputs<0.1] = 0 # cannot change into 0 first\n",
        "\n",
        "            ret_output += outputs.tolist() #cannot use int() as we need float\n",
        "\n",
        "    return ret_output #pesudo_x, pesudo_y\n",
        "'''"
      ],
      "metadata": {
        "id": "X6MW6gcr-NrW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dca625f-5db6-497e-bb37-87d17fa9ec2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport torch\\nfrom torch import nn\\nimport torch.optim as optim\\nimport torch.nn.functional as F\\nimport math\\n\\ndef got_label(batch_size, test_loader, model, device):\\n    model.eval()\\n    ret_output = []\\n    with torch.no_grad():\\n        for i, inputs in enumerate(test_loader):\\n            inputs = inputs.to(device, dtype=torch.long)\\n            outputs = model(inputs)\\n            outputs = outputs.squeeze()\\n            #outputs[outputs>=0.9] = 1 # cannot change into 1 first\\n            #outputs[outputs<0.1] = 0 # cannot change into 0 first\\n\\n            ret_output += outputs.tolist() #cannot use int() as we need float\\n\\n    return ret_output #pesudo_x, pesudo_y\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "jH-w6UfJ8xWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def testing(batch_size, test_loader, model, device):\n",
        "    model.eval()\n",
        "    ret_output = []\n",
        "    with torch.no_grad():\n",
        "        for i, inputs in enumerate(test_loader):\n",
        "            inputs = inputs.to(device, dtype=torch.long)\n",
        "            outputs = model(inputs)\n",
        "            outputs = outputs.squeeze()\n",
        "            outputs[outputs>=0.5] = 1\n",
        "            outputs[outputs<0.5] = 0\n",
        "            #print(outputs)\n",
        "            ret_output += outputs.int().tolist()\n",
        "\n",
        "    return ret_output"
      ],
      "metadata": {
        "id": "iIIxhLO980G9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameter setting + Train"
      ],
      "metadata": {
        "id": "al-yG_kp84bZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# main.py\n",
        "import os\n",
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from gensim.models import word2vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# set data path\n",
        "train_with_label = os.path.join(path_prefix, '/content/drive/MyDrive/Dissertation/Train_label.txt')\n",
        "#train_no_label = os.path.join(path_prefix, '/content/drive/MyDrive/Dissertation/Train_nolabel.txt')\n",
        "testing_data = os.path.join(path_prefix, '/content/drive/MyDrive/Dissertation/Test.txt')\n",
        "w2v_path = os.path.join(path_prefix, '/w2v_all.model')\n",
        "\n",
        "\n",
        "sen_len = 250\n",
        "fix_embedding = True # fix embedding during training\n",
        "batch_size = 96\n",
        "#batch_size = 32\n",
        "#batch_size = 64\n",
        "#epoch = 2\n",
        "epoch = 30\n",
        "lr = 0.00005 # increase learning rate\n",
        "model_dir = path_prefix\n",
        "\n",
        "print(\"loading data ...\")\n",
        "train_x, y = load_training_data(train_with_label)\n",
        "#train_x_no_label = load_training_data(train_no_label)\n",
        "\n",
        "# Preprocessing\n",
        "preprocess = Preprocess(train_x, sen_len, w2v_path=w2v_path)\n",
        "embedding = preprocess.make_embedding(load=True)\n",
        "train_x = preprocess.sentence_word2idx()\n",
        "y = preprocess.labels_to_tensor(y)\n",
        "\n",
        "\n",
        "model = GRU_Net(embedding, embedding_dim=250, hidden_dim=125, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\n",
        "model = model.to(device)\n",
        "\n",
        "#X_train, X_val, y_train, y_val = train_x[:130000], train_x[130000:], y[:130000], y[130000:]\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_x, y, test_size = 0.1, random_state = 1, stratify=y)\n",
        "\n",
        "train_dataset = TwitterDataset(X=X_train, y=y_train)\n",
        "val_dataset = TwitterDataset(X=X_val, y=y_val)\n",
        "\n",
        "# transfor data into batch of tensors\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                             batch_size = batch_size,\n",
        "                             shuffle = True,\n",
        "                             num_workers = 8)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
        "                           batch_size = batch_size,\n",
        "                           shuffle = False,\n",
        "                           num_workers = 8)\n",
        "\n",
        "#train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "#                            batch_size = batch_size,\n",
        "#                            shuffle = True,\n",
        "#                            num_workers = 0)\n",
        "\n",
        "#val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
        "#                            batch_size = batch_size,\n",
        "#                           shuffle = False,\n",
        "#                           num_workers = 0)\n",
        "\n",
        "# Begin Training\n",
        "training(batch_size, epoch, lr, model_dir, train_loader, val_loader, model, device)\n"
      ],
      "metadata": {
        "id": "qTOKtIjT88FL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44c44578-cf3b-448e-a241-af76502c6900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading data ...\n",
            "Get embedding ...\n",
            "loading word to vec model ...\n",
            "get words #5765\n",
            "total words: 5767\n",
            "\n",
            "start training, parameter total:1583251, trainable:141501\n",
            "\n",
            "[ Epoch1: 11/11 ] loss:0.633 acc:69.792 \n",
            "Train | Loss:0.65301 Acc: 67.614\n",
            "Valid | Loss:0.61267 Acc: 46.354 \n",
            "saving model with acc 46.354\n",
            "-----------------------------------------------\n",
            "[ Epoch2: 11/11 ] loss:0.517 acc:82.292 \n",
            "Train | Loss:0.60797 Acc: 74.148\n",
            "Valid | Loss:0.58743 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch3: 11/11 ] loss:0.612 acc:73.958 \n",
            "Train | Loss:0.60183 Acc: 75.095\n",
            "Valid | Loss:0.57528 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch4: 11/11 ] loss:0.527 acc:80.208 \n",
            "Train | Loss:0.59468 Acc: 74.811\n",
            "Valid | Loss:0.56857 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch5: 11/11 ] loss:0.573 acc:72.917 \n",
            "Train | Loss:0.58835 Acc: 74.905\n",
            "Valid | Loss:0.56253 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch6: 11/11 ] loss:0.572 acc:73.958 \n",
            "Train | Loss:0.58499 Acc: 74.811\n",
            "Valid | Loss:0.55707 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch7: 11/11 ] loss:0.580 acc:75.000 \n",
            "Train | Loss:0.58625 Acc: 74.716\n",
            "Valid | Loss:0.55270 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch8: 11/11 ] loss:0.623 acc:67.708 \n",
            "Train | Loss:0.58451 Acc: 74.716\n",
            "Valid | Loss:0.54836 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch9: 11/11 ] loss:0.615 acc:69.792 \n",
            "Train | Loss:0.57889 Acc: 74.811\n",
            "Valid | Loss:0.54409 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch10: 11/11 ] loss:0.593 acc:71.875 \n",
            "Train | Loss:0.57482 Acc: 74.811\n",
            "Valid | Loss:0.53991 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch11: 11/11 ] loss:0.569 acc:72.917 \n",
            "Train | Loss:0.56887 Acc: 74.811\n",
            "Valid | Loss:0.53656 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch12: 11/11 ] loss:0.627 acc:67.708 \n",
            "Train | Loss:0.56398 Acc: 74.811\n",
            "Valid | Loss:0.53360 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch13: 11/11 ] loss:0.532 acc:77.083 \n",
            "Train | Loss:0.56144 Acc: 74.811\n",
            "Valid | Loss:0.53099 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch14: 11/11 ] loss:0.666 acc:63.542 \n",
            "Train | Loss:0.56238 Acc: 74.811\n",
            "Valid | Loss:0.52750 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch15: 11/11 ] loss:0.615 acc:67.708 \n",
            "Train | Loss:0.56373 Acc: 74.811\n",
            "Valid | Loss:0.52558 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch16: 11/11 ] loss:0.618 acc:69.792 \n",
            "Train | Loss:0.55582 Acc: 74.905\n",
            "Valid | Loss:0.52426 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch17: 11/11 ] loss:0.562 acc:72.917 \n",
            "Train | Loss:0.55759 Acc: 74.811\n",
            "Valid | Loss:0.52178 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch18: 11/11 ] loss:0.574 acc:69.792 \n",
            "Train | Loss:0.55613 Acc: 74.716\n",
            "Valid | Loss:0.51917 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch19: 11/11 ] loss:0.641 acc:65.625 \n",
            "Train | Loss:0.55416 Acc: 74.811\n",
            "Valid | Loss:0.51816 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch20: 11/11 ] loss:0.567 acc:69.792 \n",
            "Train | Loss:0.55000 Acc: 74.905\n",
            "Valid | Loss:0.51734 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch21: 11/11 ] loss:0.596 acc:69.792 \n",
            "Train | Loss:0.55205 Acc: 74.811\n",
            "Valid | Loss:0.51589 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch22: 11/11 ] loss:0.488 acc:76.042 \n",
            "Train | Loss:0.55289 Acc: 74.811\n",
            "Valid | Loss:0.51476 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch23: 11/11 ] loss:0.581 acc:69.792 \n",
            "Train | Loss:0.54675 Acc: 74.811\n",
            "Valid | Loss:0.51387 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch24: 11/11 ] loss:0.520 acc:76.042 \n",
            "Train | Loss:0.53928 Acc: 74.811\n",
            "Valid | Loss:0.51409 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch25: 11/11 ] loss:0.562 acc:70.833 \n",
            "Train | Loss:0.55133 Acc: 74.811\n",
            "Valid | Loss:0.51205 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch26: 11/11 ] loss:0.493 acc:77.083 \n",
            "Train | Loss:0.54893 Acc: 74.716\n",
            "Valid | Loss:0.51213 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch27: 11/11 ] loss:0.469 acc:77.083 \n",
            "Train | Loss:0.54683 Acc: 74.811\n",
            "Valid | Loss:0.51094 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch28: 11/11 ] loss:0.531 acc:76.042 \n",
            "Train | Loss:0.54962 Acc: 74.811\n",
            "Valid | Loss:0.51099 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch29: 11/11 ] loss:0.566 acc:72.917 \n",
            "Train | Loss:0.54861 Acc: 74.811\n",
            "Valid | Loss:0.51057 Acc: 46.354 \n",
            "-----------------------------------------------\n",
            "[ Epoch30: 11/11 ] loss:0.560 acc:73.958 \n",
            "Train | Loss:0.55041 Acc: 74.811\n",
            "Valid | Loss:0.50998 Acc: 46.354 \n",
            "-----------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "### Get pesudo label\n",
        "train_x_no_label_word = load_training_data(train_no_label)\n",
        "train_x_no_label = preprocess.sentence_word2idx()\n",
        "preprocess = Preprocess(train_x_no_label, sen_len, w2v_path=w2v_path)\n",
        "embedding = preprocess.make_embedding(load=True)\n",
        "train_x_no_label_dataset = TwitterDataset(X=train_x_no_label, y=None)\n",
        "\n",
        "train_no_loader = torch.utils.data.DataLoader(dataset = train_x_no_label_dataset,\n",
        "                            batch_size = batch_size,\n",
        "                            shuffle = False,\n",
        "                            num_workers = 0)\n",
        "\n",
        "model = torch.load(os.path.join(model_dir, 'ckpt.model'))\n",
        "output = got_label(batch_size, train_no_loader, model, device)\n",
        "'''"
      ],
      "metadata": {
        "id": "7uGKqXED-cdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc786b5-f3f9-4a56-ba89-5bc431659220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n### Get pesudo label\\ntrain_x_no_label_word = load_training_data(train_no_label)\\ntrain_x_no_label = preprocess.sentence_word2idx()\\npreprocess = Preprocess(train_x_no_label, sen_len, w2v_path=w2v_path)\\nembedding = preprocess.make_embedding(load=True)\\ntrain_x_no_label_dataset = TwitterDataset(X=train_x_no_label, y=None)\\n\\ntrain_no_loader = torch.utils.data.DataLoader(dataset = train_x_no_label_dataset,\\n                            batch_size = batch_size,\\n                            shuffle = False,\\n                            num_workers = 0)\\n\\nmodel = torch.load(os.path.join(model_dir, 'ckpt.model'))\\noutput = got_label(batch_size, train_no_loader, model, device)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "### Pesudo labelled sample selection\n",
        "#import Tensor\n",
        "\n",
        "pesudo_label_index = []\n",
        "\n",
        "#pesudo_x = preprocess.sentence_word2idx()\n",
        "#preprocess = Preprocess(pesudo_x, sen_len, w2v_path=w2v_path)\n",
        "#embedding = preprocess.make_embedding(load=True)\n",
        "\n",
        "for i in range(len(output)):\n",
        "  if output[i] >= 0.9 or output[i] <= 0.1:\n",
        "  #if output[i] >= 0.8:\n",
        "    pesudo_label_index.append(i)\n",
        "\n",
        "pesudo_x = [train_x_no_label[i] for i in pesudo_label_index]\n",
        "pesudo_y = [output[i] for i in pesudo_label_index]\n",
        "\n",
        "for i in range(len(pesudo_y)):\n",
        "  if pesudo_y[i] <= 0.3:\n",
        "    pesudo_y[i] = 0.0\n",
        "  else:\n",
        "    pesudo_y[i] = 1.0\n",
        "\n",
        "#print(pesudo_x)\n",
        "\n",
        "#pesudo_x = TwitterDataset(pesudo_x,  y=None)\n",
        "#pesudo_x = torch.Tensor(pesudo_x)\n",
        "'''"
      ],
      "metadata": {
        "id": "aw0-VD38-fAD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e9f68cf-2e07-4c2d-f863-b3044d4aaa6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n### Pesudo labelled sample selection\\n#import Tensor\\n\\npesudo_label_index = []\\n\\n#pesudo_x = preprocess.sentence_word2idx()\\n#preprocess = Preprocess(pesudo_x, sen_len, w2v_path=w2v_path)\\n#embedding = preprocess.make_embedding(load=True)\\n\\nfor i in range(len(output)):\\n  if output[i] >= 0.9 or output[i] <= 0.1:\\n  #if output[i] >= 0.8:\\n    pesudo_label_index.append(i)\\n\\npesudo_x = [train_x_no_label[i] for i in pesudo_label_index]\\npesudo_y = [output[i] for i in pesudo_label_index]\\n\\nfor i in range(len(pesudo_y)):\\n  if pesudo_y[i] <= 0.3:\\n    pesudo_y[i] = 0.0\\n  else:\\n    pesudo_y[i] = 1.0\\n\\n#print(pesudo_x)\\n\\n#pesudo_x = TwitterDataset(pesudo_x,  y=None)\\n#pesudo_x = torch.Tensor(pesudo_x)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "print(\"loading data ...\")\n",
        "train_x, y = load_training_data(train_with_label)\n",
        "preprocess = Preprocess(train_x, sen_len, w2v_path=w2v_path)\n",
        "embedding = preprocess.make_embedding(load=True)\n",
        "train_x = preprocess.sentence_word2idx()\n",
        "y = preprocess.labels_to_tensor(y)\n",
        "\n",
        "model = GRU_Net(embedding, embedding_dim=250, hidden_dim=125, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\n",
        "model = model.to(device)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_x, y, test_size = 0.05, random_state = 10, stratify=y)\n",
        "\n",
        "# Combine labelled samples with pesudo labelled samples\n",
        "\n",
        "pesudo_x = torch.tensor([item.cpu().detach().numpy() for item in pesudo_x]).cuda() #turn list to tensor\n",
        "pesudo_x = pesudo_x.to(device, dtype=torch.long) # set device \"cuda\"\n",
        "X_train = X_train.to(device, dtype=torch.long) # turn list to tensor\n",
        "retrain_x = torch.cat((X_train,pesudo_x),0)  # combine with label and without label dataset into one\n",
        "pesudo_y = preprocess.labels_to_tensor(pesudo_y) #turn list to tensor\n",
        "pesudo_y = torch.Tensor(pesudo_y) # turn list to tensor\n",
        "pesudo_y = pesudo_y.to(device, dtype=torch.float) # set device \"cuda\"\n",
        "y_train = y_train.to(device, dtype=torch.float) # set device \"cuda\"\n",
        "retrain_y = torch.cat((y_train,pesudo_y),0)  # combine with label and without label dataset into one\n",
        "'''"
      ],
      "metadata": {
        "id": "6Uxv1sU9-hH8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78107784-5407-470a-d1a0-bc6326be18e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprint(\"loading data ...\")\\ntrain_x, y = load_training_data(train_with_label)\\npreprocess = Preprocess(train_x, sen_len, w2v_path=w2v_path)\\nembedding = preprocess.make_embedding(load=True)\\ntrain_x = preprocess.sentence_word2idx()\\ny = preprocess.labels_to_tensor(y)\\n\\nmodel = GRU_Net(embedding, embedding_dim=250, hidden_dim=125, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\\nmodel = model.to(device)\\n\\nX_train, X_val, y_train, y_val = train_test_split(train_x, y, test_size = 0.05, random_state = 10, stratify=y)\\n\\n# Combine labelled samples with pesudo labelled samples\\n\\npesudo_x = torch.tensor([item.cpu().detach().numpy() for item in pesudo_x]).cuda() #turn list to tensor\\npesudo_x = pesudo_x.to(device, dtype=torch.long) # set device \"cuda\"\\nX_train = X_train.to(device, dtype=torch.long) # turn list to tensor\\nretrain_x = torch.cat((X_train,pesudo_x),0)  # combine with label and without label dataset into one\\npesudo_y = preprocess.labels_to_tensor(pesudo_y) #turn list to tensor\\npesudo_y = torch.Tensor(pesudo_y) # turn list to tensor\\npesudo_y = pesudo_y.to(device, dtype=torch.float) # set device \"cuda\"\\ny_train = y_train.to(device, dtype=torch.float) # set device \"cuda\"\\nretrain_y = torch.cat((y_train,pesudo_y),0)  # combine with label and without label dataset into one\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(pesudo_y)"
      ],
      "metadata": {
        "id": "mhTpdKPjPv-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "### Retrain model with pesudo labelled samples\n",
        "#batch_size = 192\n",
        "batch_size = 96\n",
        "#batch_size = 32\n",
        "#epoch = 2\n",
        "#epoch = 5\n",
        "epoch = 50\n",
        "#epoch = 7\n",
        "lr = 0.00005 # use a smaller learning rate\n",
        "\n",
        "retrain_dataset = TwitterDataset(X=retrain_x, y=retrain_y)\n",
        "val_dataset = TwitterDataset(X=X_val, y=y_val)\n",
        "\n",
        "# transfor data into batch of tensors\n",
        "retrain_loader = torch.utils.data.DataLoader(dataset = retrain_dataset,\n",
        "                          batch_size = batch_size,\n",
        "                          shuffle = True,\n",
        "                          num_workers = 0)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
        "                      batch_size = batch_size,\n",
        "                      shuffle = False,\n",
        "                      num_workers = 0)\n",
        "\n",
        "training(batch_size, epoch, lr, model_dir, retrain_loader, val_loader, model, device)\n",
        "'''"
      ],
      "metadata": {
        "id": "sj9xnNsl-jAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d098d660-2731-46de-d5df-b691fa337be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n### Retrain model with pesudo labelled samples\\n#batch_size = 192\\nbatch_size = 96\\n#batch_size = 32\\n#epoch = 2\\n#epoch = 5\\nepoch = 50\\n#epoch = 7\\nlr = 0.00005 # use a smaller learning rate\\n\\nretrain_dataset = TwitterDataset(X=retrain_x, y=retrain_y)\\nval_dataset = TwitterDataset(X=X_val, y=y_val)\\n\\n# transfor data into batch of tensors\\nretrain_loader = torch.utils.data.DataLoader(dataset = retrain_dataset,\\n                          batch_size = batch_size,\\n                          shuffle = True,\\n                          num_workers = 0)\\n\\nval_loader = torch.utils.data.DataLoader(dataset = val_dataset,\\n                      batch_size = batch_size,\\n                      shuffle = False,\\n                      num_workers = 0)\\n\\ntraining(batch_size, epoch, lr, model_dir, retrain_loader, val_loader, model, device)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict and save to csv file"
      ],
      "metadata": {
        "id": "g4eX2bMs9LNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"loading testing data ...\")\n",
        "test_x = load_testing_data(testing_data)\n",
        "preprocess = Preprocess(test_x, sen_len, w2v_path=w2v_path)\n",
        "embedding = preprocess.make_embedding(load=True)\n",
        "test_x = preprocess.sentence_word2idx()\n",
        "test_dataset = TwitterDataset(X=test_x, y=None)\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                            batch_size = batch_size,\n",
        "                            shuffle = False,\n",
        "                            num_workers = 8)\n",
        "\n",
        "#test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "#                           batch_size = batch_size,\n",
        "#                           shuffle = False,\n",
        "#                           num_workers = 0)\n",
        "print('\\nload model ...')\n",
        "model = torch.load(os.path.join(model_dir, '/content/ckpt.model'))\n",
        "outputs = testing(batch_size, test_loader, model, device)\n",
        "\n",
        "# save as csv\n",
        "tmp = pd.DataFrame({\"id\":[str(i) for i in range(len(test_x))],\"labels\":outputs})\n",
        "print(\"save csv ...\")\n",
        "tmp.to_csv(os.path.join(path_prefix, 'Dissertation_predict.csv'), index=False)\n",
        "tmp.to_csv('/content/drive/My Drive/Dissertation/Dissertation_predict_comparsion_gru_supervised_75%.csv', index=False)\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.flush_and_unmount()\n",
        "\n",
        "print(\"Finish Predicting\")"
      ],
      "metadata": {
        "id": "qmbBBo0a9IJk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d00da557-5b46-4061-d917-793d0b3a17ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading testing data ...\n",
            "Get embedding ...\n",
            "loading word to vec model ...\n",
            "get words #5765\n",
            "total words: 5767\n",
            "sentence count #671\n",
            "load model ...\n",
            "save csv ...\n",
            "Finish Predicting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tmp.loc[tmp['labels'] == 1])\n",
        "print(len(tmp.loc[tmp['labels'] == 1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXvg8Znd94Z9",
        "outputId": "541fd453-5ce3-4bdd-ab3e-d9131f2cd6cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id  labels\n",
            "0      0       1\n",
            "1      1       1\n",
            "2      2       1\n",
            "3      3       1\n",
            "4      4       1\n",
            "..   ...     ...\n",
            "666  666       1\n",
            "667  667       1\n",
            "668  668       1\n",
            "669  669       1\n",
            "670  670       1\n",
            "\n",
            "[671 rows x 2 columns]\n",
            "671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_verification = pd.read_excel('/content/drive/MyDrive/Dissertation/test_verification.xlsx')\n",
        "y_true = test_verification['labels']\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "print(f1_score(y_true, tmp['labels'], average='binary'))\n",
        "\n",
        "print(len(y_true))\n",
        "print(len(tmp['labels']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsYLVixRD_IJ",
        "outputId": "618c99b6-40e6-4649-f2c3-7722c9841940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.16666666666666669\n",
            "671\n",
            "671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#drive.flush_and_unmount()\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_true, tmp['labels']))"
      ],
      "metadata": {
        "id": "bg_TfDArKZMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0ebc1b7-b073-4573-8d55-0da8c70d7739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       610\n",
            "           1       0.09      1.00      0.17        61\n",
            "\n",
            "    accuracy                           0.09       671\n",
            "   macro avg       0.05      0.50      0.08       671\n",
            "weighted avg       0.01      0.09      0.02       671\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kYmuGbbfv9fC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}